\chapter{Introduction}

Many program perform queries on collections of data, and for non-trivial amounts
of data it is useful to execute the queries efficiently. When the data is
updated often enough, it can also be useful to update the results of some
queries \emph{incrementally} whenever the input changes, so that up-to-date
results are quickly available, even for queries that are expensive to execute.

For instance, a program manipulating anagraphic data about citizens of a country
might need to compute statistics on them, such as their average age, and update
those statistics when the set of citizens changes.

Traditional relational database management systems (RDBMS) support both queries
optimization and (in quite a few cases) incremental update of query results
(called there \emph{incremental view maintenance}).

However, often queries are executed on collections of data that are not stored
in a database, but in collections manipulated by some program. Moving in-memory
data to RDBMSs typically does not improve
performance~\citep{Stonebraker07,Rompf2015functional}, and reusing database
optimizers is not trivial.

Moreover, many programming languages are far more expressive than RDBMSs.
Typical RDBMS can only manipulate SQL relations, that is multisets (or bags) of
tuples (or sometimes sets of tuples, after duplicate elimination). Typical
programming languages (PL) support also arbitrarily nested lists and maps of
data, and allow programmers to define new data types with few restrictions; a
typical PL will also allow a far richer set of operations than SQL\@.

However, typical PLs do not apply typical database optimizations to collection
queries, and if queries are incrementalized, this is often done by hand, even
though code implementing incremental query is error-prone and
hard-to-maintain~\citep{Salvaneschi13reactive}.

What's worse, some of these manual optimizations are best done over the whole
program. Some optimizations only become possible after inlining, which breaks
modularity if done by hand.
Worse, adding an index on some collection can speed up looking
for some information, but each index must be maintained incrementally when the
underlying data changes. Depending on the actual queries and updates performed
on the collection, and on how often they happen, it might turn out that updating
an index takes more time than the index saves; hence the choice of which indexes
to enable depends on the whole program. However, adding/removing an index
requires updating all PL queries to use it, while RDBMS queries can use an index
transparently.

Overall, manual optimizations are not only effort-intensive and error-prone, but
they also significantly reduce modularity.

\section{This thesis}
To reduce the need for manual optimizations, in this thesis we propose
techniques for optimizing higher-order collection queries and executing them
incrementally.
\pg{WIP: What's my thesis? ``We provide approaches'' is better but not yet enough.}
By generalizing database-inspired approaches, we provide approaches to query optimization and
incrementalization by code transformation to apply to higher-order collection
queries including user-defined functions.
%While
\pg{Status: others have done much more on optimization.}
Instead of building on existing approaches to incrementalization, such as
self-adjusting computation~\citep{Acar09}, we introduce a novel
incrementalization approach which enables further transformations on the
incrementalization results. This approach is in fact not restricted to
collection queries but applicable to other domains, but it requires adaptation
for the domain under consideration.
%
Further research is needed, but a number of case studies suggest
applicability, even in some scenarios beyond existing
techniques~\citep{Koch2016incremental}.

We consider the problem for functional programming languages such as Haskell or
Scala, and we consider collection queries written using the APIs of their
collection libraries, which we treat as an embedded domain-specific language
(EDSL). Such APIs (or DSLs) contain powerful operators on collections such as
$\Varid{map}$,
which are higher-order, that
is they take as arguments arbitrary functions in the host language that we must
also handle.

Therefore, our optimizations and incrementalizations must handle programs in
higher-order EDSLs that can contain arbitrary code in the host language. Hence,
many of our optimizations will exploit on properties of our collection EDSL, but
will need to handle host language code. We restrict the problem to purely
functional programs (without mutation or other side effects, mostly including
non-termination), because such programs can be more ``declarative'' and because
avoiding side effects can enable more powerful optimization and simplify the
work of the optimizer at the same time.

This thesis is divided into two parts:
\begin{itemize}
\item In \cref{part:ch-aosd13}, we optimize collection
  queries by static program transformation, based on a set of rewrite
  rules~\citep{GiarrussoAOSD13}.
\item In \cref{part:incr}, we incrementalize programs by transforming them to
  new programs. This thesis presents the first approach that
  handles higher-order programs through program transformation; hence, while
  our main examples use collection queries, we phrase the work in terms of
  $\lambda$-calculi with unspecified primitives~\citep*{CaiEtAl2014ILC}.
  In \cref{ch:cts}, we extend ILC with a further program transformation
  step, so that base programs can store intermediate results and derivatives can
  reuse them, but without resorting to dynamic memoization and necessarily
  needing to look results up at runtime. To this end, we build on work by
  \citet{Liu00} and extend it to a higher-order, typed setting.
\end{itemize}

\Cref{part:incr} is more theoretical than
\cref{part:ch-aosd13}, because optimizations in
\cref{part:ch-aosd13} are much better understood than our approach to
incrementalization.

To incrementalize programs, we are the first to extend to higher-order programs
techniques based on finite differencing for queries on
collections~\citep{Paige82FDC} and
databases~\citep{Blakeley:1986:EUM,Gupta99MMV}.
Incrementalizing by finite differencing is a well-understood technique for
database queries. How to generalize it for higher-order programs or beyond
databases was less clear, so we spend significant energy on providing sound
mathematical foundations for this transformation.

In fact, it took us a while to be sure that our transformation was correct, and
to understand why; our first correctness proof~\citep*{CaiEtAl2014ILC}, while a
significant step, was still more complex than needed. In \cref{part:incr},
especially \cref{ch:derive-formally}, we offer a mathematically much simpler
proof.

Contributions of \cref{part:ch-aosd13} are listed in \cref{sec:aosd13-contributions}.
Contributions of \cref{part:incr} are listed at the end of
\cref{sec:incr-contributions}.
 %\Cref{part:ch-aosd13} makes the following contributions:
%\pg{Contributions: maybe in first chapters?} % Yes

\subsection{Included papers}
This thesis includes material from joint work with colleagues.

\Cref{part:ch-aosd13} is based on work by \citet*{GiarrussoAOSD13}. While the
work was in collaboration, a few clearer responsibilities arose.
I did most of the implementation work, and collaborated to the writing: among
other things I devised the embedding for collection operations, implemented
optimizations and indexing, implemented compilation when interpretation did not
achieve sufficient performance, and performed the performance evaluation.
Michael Eichberg and Ralf Mitschke contributed to the evaluation by adapting
FindBugs queries.
Christian K{\"{a}}stner contributed, among other things, to the experiment
design for the performance evaluation. Klaus Ostermann proposed the original
idea (together with an initial prototype) and supervised the project.

The first chapters of \cref{part:incr} are originally based on work by \citet*{CaiEtAl2014ILC}, though
significantly revised; only \cref{sec:applying,sec:rw} survive mostly unchanged.
This work was even more of a team effort. I initiated and led the
overall project and came up with the original notion of change structures; Cai
Yufei contributed differentiation itself and its first correctness proof.
Tillmann Rendel contributed significant simplifications and started the overall
mechanization effort.
\Cref{part:incr} contains novel correctness proof; its history and
contributions are discussed in \cref{sec:ilc-dev-history}.

Furthermore, \cref{ch:cts} comes from a recently submitted
manuscript~\citep*{Giarrusso2018Static}. I
designed the overall approach, the transformation and the case study on
sequences and nested loops. Proofs were done in collaboration with Yann
RÃ©gis-Gianas: most of the actual mechanization work is his, though I contributed significantly to the
correctness proof for ILC\@. Philipp Schuster contributed to the evaluation,
devising language plugins for bags and maps in collaboration with me.

\pg{Readd section on excluded papers.}

\subsection{Excluded papers}
This thesis improves modularity of collection queries by automating different sorts of
optimizations on them.

During my PhD work I collaborated on several other papers, mostly on different facets of
modularity, which do not appear in this thesis.

\paragraph{Modularity}
Module boundaries hide information on implementation that is not relevant to
clients. However, it often turns out that some scenarios, clients or task
require such hidden information. As discussed, manual optimization requires
hidden information: hence, researchers strive to automate optimizations. But
other tasks often violate these boundaries. I collaborated on research on
understanding why this happens and how to deal with this
problem~\citep*{OGKR:ECOOP11}.

\paragraph{Software Product Lines}
In some scenarios, a different form of modular software is realized through
software product lines (SPLs), where many \emph{variants} of a single software
artifact (with different sets of features) can be generated. This is often done
using conditional compilation, for instance through the C preprocessor.

But if a software product line uses conditional compilation, processing
automatically its source code is difficult---in fact, even lexing or parsing it
is a challenge. Since the number of variants is exponential in the number of
features, generating and processing each variant does not scale.

To address these
challenges, I collaborated with the TypeChef project to develop a
variability-aware lexer \citep*{KPO:VaMoS11} and parser
\citep*{KGREOB:OOPSLA11}.

Another challenge is predicting non-functional properties (such as code size or
performance) of a variant of a software product line before generation and
direct measurement. Such predictions can be useful to select efficiently a
variant that satisfies some non-functional requirements.
I collaborated to research on predicting such non-functional properties: we
measure the properties on a few variants and extrapolate the results to other
variants. In particular, I collaborated to the experimental evaluation on a few
open source projects, where even a simple model reached significant accuracy in a
few scenarios \citep*{SRKGAK:SPLC11,siegmund2013scalable}.

\paragraph{Domain-specific languages}
This thesis is concerned with DSLs, both ones for the domain of collection
queries, but also (in \cref{part:incr}) more general ones.

Different forms of language composition, both among DSLs and across
general-purpose languages and DSLs are possible, but their relationship is
nontrivial; I collaborated to work classifying the different forms of language
compositions~\citep*{ErdwegGR12}.

The implementation of \LoS, as described in \cref{part:ch-aosd13}, requires an
extensible representation of ASTs, similar to the one used by LMS
\citep{rompf2010lightweight}. While this representation is pretty flexible, it
relies on Scala's support of GADTs \citep{Emir06Variance,Emir07Patterns}, which
is known to be somewhat fragile due to implementation bugs. In fact,
sound pattern matching on Scala's extensible GADTs is impossible
without imposing significant restrictions to extensibility, due to language
extensions not considered during formal study
\citep{Emir06Variance,Emir07Correct}: the problem arises due to the interaction
between GADTs, declaration-site variance annotations and variant refinement of
type arguments at inheritance time.
To illustrate the problem, I've shown it already applies to an extensible
definitional interpreter for $\lambda_{<:}$~\citep{Giarrusso2013gadt}.
While solutions have been investigated in other settings
\citep{Scherer13GADTSub}, the problem remains open for Scala to this day.

%\pg{TODO maybe manuscript on cake pattern?}.

\subsection{Navigating this thesis}
The two parts of this thesis, while related by the common topic of collection
queries, can be read mostly independently from each other.
Summaries of the two parts are given respectively in
\cref{sec:navigating-aosd13,sec:navigating-incr}.

% Many chapters are independent from each other, with a few exceptions.
% \Cref{part:ch-aosd13} can be read independently from the rest.
% % Write this even though maybe I restructure the parts.
% \Cref{part:incr} describes incrementalization.

\paragraph{Numbering}
We use numbering and hyperlinks to help navigation; we explain our conventions
to enable exploiting them.

To simplify navigation, we number all sorts of ``theorems'' (including here
definitions, remarks, even examples or descriptions of notation)
per-chapter with counters including section numbers. For
instance, \cref{def:inc-semantics} is the first such item in
\cref{ch:derive-formally}, followed by \cref{thm:derive-correct}, and they both
appear in \cref{sec:correct-derive}. And we can be sure that \cref{lem:derive-typing}
comes after both ``theorems'' because of its number, even if they are of different sorts.

Similarly, we number tables and figures per-chapter with a shared counter. For instance,
\cref{fig:measurements-overview} is the first figure or table in \cref{sec:evaluation},
followed by \cref{table:performance}.

To help reading, at times we will repeat or anticipate statements of
``theorems'' to refer to them, without forcing readers to jump back and forth.
We will then reuse the original number.
For instance \cref{sec:correct-derive} contains a copy of \cref{slogan:derive}
with its original number.

For ease of navigation, all such references are hyperlinked in electronic
versions of this thesis, and the table of contents is exposed to PDF readers via
PDF bookmarks.

% Modern libraries offer high-level APIs for manipulating (in-memory) collections;
% equivalently, such an API is an embedded domain-specific language (EDSL).

% Compared to SQL, EDSL queries will typically be able to use user-defined
% functions in queries. Moreover, the author of EDSL queries can enjoy abstraction
% mechanisms from the underlying language.

% \pg{Examples!}

% Collection operations are typically executed directly; however, they can instead
% construct a query representation that can then be optimized.

% \section{Our design for incrementalizing queries}
% We designed our incrementalization system by abstracting from needs and ideas for
% collection APIs.
% \begin{itemize}
% \item collection APIs are typically higher-order. This allows further
%   flexibility compared to first-order query languages: in particular we can
%   design new operators in terms of existing ones.
% \item many collection types support equations that can be used for optimization.
%   For instance
% \end{itemize}

% \subsection{Domain-specific}
% Special optimizations are possible because many collection datatypes, equality
% is not purely structural. Two lists are equal if they are structurally equal.
% But two sets are equal if they have the same elements, which does not imply they
% are structurally equal.\footnote{\pg{Cool but not necessarily appropriate here.}
%   Technically, the datatypes are not freely generated and their signatures
%   contain further equations.}
% %
% A general-purpose optimizer cannot exploit this to return a structurally
% different set with ``the same meaning'', but a domain-specific optimizer can be
% instructed to do so.

% Taking this into account we were led to design our incrementalization system to
% allow domain-specific support from the start.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis-main"
%%% End:
