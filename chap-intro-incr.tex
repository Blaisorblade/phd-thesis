% Emacs, this is -*- latex -*-!
%% ODER: format ==         = "\mathrel{==}"
%% ODER: format /=         = "\neq "
%
%
\makeatletter
\@ifundefined{lhs2tex.lhs2tex.sty.read}%
  {\@namedef{lhs2tex.lhs2tex.sty.read}{}%
   \newcommand\SkipToFmtEnd{}%
   \newcommand\EndFmtInput{}%
   \long\def\SkipToFmtEnd#1\EndFmtInput{}%
  }\SkipToFmtEnd

\newcommand\ReadOnlyOnce[1]{\@ifundefined{#1}{\@namedef{#1}{}}\SkipToFmtEnd}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{stmaryrd}
\DeclareFontFamily{OT1}{cmtex}{}
\DeclareFontShape{OT1}{cmtex}{m}{n}
  {<5><6><7><8>cmtex8
   <9>cmtex9
   <10><10.95><12><14.4><17.28><20.74><24.88>cmtex10}{}
\DeclareFontShape{OT1}{cmtex}{m}{it}
  {<-> ssub * cmtt/m/it}{}
\newcommand{\texfamily}{\fontfamily{cmtex}\selectfont}
\DeclareFontShape{OT1}{cmtt}{bx}{n}
  {<5><6><7><8>cmtt8
   <9>cmbtt9
   <10><10.95><12><14.4><17.28><20.74><24.88>cmbtt10}{}
\DeclareFontShape{OT1}{cmtex}{bx}{n}
  {<-> ssub * cmtt/bx/n}{}
\newcommand{\tex}[1]{\text{\texfamily#1}}	% NEU

\newcommand{\Sp}{\hskip.33334em\relax}


\newcommand{\Conid}[1]{\mathit{#1}}
\newcommand{\Varid}[1]{\mathit{#1}}
\newcommand{\anonymous}{\kern0.06em \vbox{\hrule\@width.5em}}
\newcommand{\plus}{\mathbin{+\!\!\!+}}
\newcommand{\bind}{\mathbin{>\!\!\!>\mkern-6.7mu=}}
\newcommand{\rbind}{\mathbin{=\mkern-6.7mu<\!\!\!<}}% suggested by Neil Mitchell
\newcommand{\sequ}{\mathbin{>\!\!\!>}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\usepackage{polytable}

%mathindent has to be defined
\@ifundefined{mathindent}%
  {\newdimen\mathindent\mathindent\leftmargini}%
  {}%

\def\resethooks{%
  \global\let\SaveRestoreHook\empty
  \global\let\ColumnHook\empty}
\newcommand*{\savecolumns}[1][default]%
  {\g@addto@macro\SaveRestoreHook{\savecolumns[#1]}}
\newcommand*{\restorecolumns}[1][default]%
  {\g@addto@macro\SaveRestoreHook{\restorecolumns[#1]}}
\newcommand*{\aligncolumn}[2]%
  {\g@addto@macro\ColumnHook{\column{#1}{#2}}}

\resethooks

\newcommand{\onelinecommentchars}{\quad-{}- }
\newcommand{\commentbeginchars}{\enskip\{-}
\newcommand{\commentendchars}{-\}\enskip}

\newcommand{\visiblecomments}{%
  \let\onelinecomment=\onelinecommentchars
  \let\commentbegin=\commentbeginchars
  \let\commentend=\commentendchars}

\newcommand{\invisiblecomments}{%
  \let\onelinecomment=\empty
  \let\commentbegin=\empty
  \let\commentend=\empty}

\visiblecomments

\newlength{\blanklineskip}
\setlength{\blanklineskip}{0.66084ex}

\newcommand{\hsindent}[1]{\quad}% default is fixed indentation
\let\hspre\empty
\let\hspost\empty
\newcommand{\NB}{\textbf{NB}}
\newcommand{\Todo}[1]{$\langle$\textbf{To do:}~#1$\rangle$}

\EndFmtInput
\makeatother
%
%
%
%
%
%
% This package provides two environments suitable to take the place
% of hscode, called "plainhscode" and "arrayhscode". 
%
% The plain environment surrounds each code block by vertical space,
% and it uses \abovedisplayskip and \belowdisplayskip to get spacing
% similar to formulas. Note that if these dimensions are changed,
% the spacing around displayed math formulas changes as well.
% All code is indented using \leftskip.
%
% Changed 19.08.2004 to reflect changes in colorcode. Should work with
% CodeGroup.sty.
%
\ReadOnlyOnce{polycode.fmt}%
\makeatletter

\newcommand{\hsnewpar}[1]%
  {{\parskip=0pt\parindent=0pt\par\vskip #1\noindent}}

% can be used, for instance, to redefine the code size, by setting the
% command to \small or something alike
\newcommand{\hscodestyle}{}

% The command \sethscode can be used to switch the code formatting
% behaviour by mapping the hscode environment in the subst directive
% to a new LaTeX environment.

\newcommand{\sethscode}[1]%
  {\expandafter\let\expandafter\hscode\csname #1\endcsname
   \expandafter\let\expandafter\endhscode\csname end#1\endcsname}

% "compatibility" mode restores the non-polycode.fmt layout.

\newenvironment{compathscode}%
  {\par\noindent
   \advance\leftskip\mathindent
   \hscodestyle
   \let\\=\@normalcr
   \let\hspre\(\let\hspost\)%
   \pboxed}%
  {\endpboxed\)%
   \par\noindent
   \ignorespacesafterend}

\newcommand{\compaths}{\sethscode{compathscode}}

% "plain" mode is the proposed default.
% It should now work with \centering.
% This required some changes. The old version
% is still available for reference as oldplainhscode.

\newenvironment{plainhscode}%
  {\hsnewpar\abovedisplayskip
   \advance\leftskip\mathindent
   \hscodestyle
   \let\hspre\(\let\hspost\)%
   \pboxed}%
  {\endpboxed%
   \hsnewpar\belowdisplayskip
   \ignorespacesafterend}

\newenvironment{oldplainhscode}%
  {\hsnewpar\abovedisplayskip
   \advance\leftskip\mathindent
   \hscodestyle
   \let\\=\@normalcr
   \(\pboxed}%
  {\endpboxed\)%
   \hsnewpar\belowdisplayskip
   \ignorespacesafterend}

% Here, we make plainhscode the default environment.

\newcommand{\plainhs}{\sethscode{plainhscode}}
\newcommand{\oldplainhs}{\sethscode{oldplainhscode}}
\plainhs

% The arrayhscode is like plain, but makes use of polytable's
% parray environment which disallows page breaks in code blocks.

\newenvironment{arrayhscode}%
  {\hsnewpar\abovedisplayskip
   \advance\leftskip\mathindent
   \hscodestyle
   \let\\=\@normalcr
   \(\parray}%
  {\endparray\)%
   \hsnewpar\belowdisplayskip
   \ignorespacesafterend}

\newcommand{\arrayhs}{\sethscode{arrayhscode}}

% The mathhscode environment also makes use of polytable's parray 
% environment. It is supposed to be used only inside math mode 
% (I used it to typeset the type rules in my thesis).

\newenvironment{mathhscode}%
  {\parray}{\endparray}

\newcommand{\mathhs}{\sethscode{mathhscode}}

% texths is similar to mathhs, but works in text mode.

\newenvironment{texthscode}%
  {\(\parray}{\endparray\)}

\newcommand{\texths}{\sethscode{texthscode}}

% The framed environment places code in a framed box.

\def\codeframewidth{\arrayrulewidth}
\RequirePackage{calc}

\newenvironment{framedhscode}%
  {\parskip=\abovedisplayskip\par\noindent
   \hscodestyle
   \arrayrulewidth=\codeframewidth
   \tabular{@{}|p{\linewidth-2\arraycolsep-2\arrayrulewidth-2pt}|@{}}%
   \hline\framedhslinecorrect\\{-1.5ex}%
   \let\endoflinesave=\\
   \let\\=\@normalcr
   \(\pboxed}%
  {\endpboxed\)%
   \framedhslinecorrect\endoflinesave{.5ex}\hline
   \endtabular
   \parskip=\belowdisplayskip\par\noindent
   \ignorespacesafterend}

\newcommand{\framedhslinecorrect}[2]%
  {#1[#2]}

\newcommand{\framedhs}{\sethscode{framedhscode}}

% The inlinehscode environment is an experimental environment
% that can be used to typeset displayed code inline.

\newenvironment{inlinehscode}%
  {\(\def\column##1##2{}%
   \let\>\undefined\let\<\undefined\let\\\undefined
   \newcommand\>[1][]{}\newcommand\<[1][]{}\newcommand\\[1][]{}%
   \def\fromto##1##2##3{##3}%
   \def\nextline{}}{\) }%

\newcommand{\inlinehs}{\sethscode{inlinehscode}}

% The joincode environment is a separate environment that
% can be used to surround and thereby connect multiple code
% blocks.

\newenvironment{joincode}%
  {\let\orighscode=\hscode
   \let\origendhscode=\endhscode
   \def\endhscode{\def\hscode{\endgroup\def\@currenvir{hscode}\\}\begingroup}
   %\let\SaveRestoreHook=\empty
   %\let\ColumnHook=\empty
   %\let\resethooks=\empty
   \orighscode\def\hscode{\endgroup\def\@currenvir{hscode}}}%
  {\origendhscode
   \global\let\hscode=\orighscode
   \global\let\endhscode=\origendhscode}%

\makeatother
\EndFmtInput
%
%
%
% First, let's redefine the forall, and the dot.
%
%
% This is made in such a way that after a forall, the next
% dot will be printed as a period, otherwise the formatting
% of `comp_` is used. By redefining `comp_`, as suitable
% composition operator can be chosen. Similarly, period_
% is used for the period.
%
\ReadOnlyOnce{forall.fmt}%
\makeatletter

% The HaskellResetHook is a list to which things can
% be added that reset the Haskell state to the beginning.
% This is to recover from states where the hacked intelligence
% is not sufficient.

\let\HaskellResetHook\empty
\newcommand*{\AtHaskellReset}[1]{%
  \g@addto@macro\HaskellResetHook{#1}}
\newcommand*{\HaskellReset}{\HaskellResetHook}

\global\let\hsforallread\empty

\newcommand\hsforall{\global\let\hsdot=\hsperiodonce}
\newcommand*\hsperiodonce[2]{#2\global\let\hsdot=\hscompose}
\newcommand*\hscompose[2]{#1}

\AtHaskellReset{\global\let\hsdot=\hscompose}

% In the beginning, we should reset Haskell once.
\HaskellReset

\makeatother
\EndFmtInput


% https://github.com/conal/talk-2015-essence-and-origins-of-frp/blob/master/mine.fmt
% Complexity notation:






% If an argument to a formatting directive starts with let, lhs2TeX likes to
% helpfully prepend a space to the let, even though that's seldom desirable.
% Write lett to prevent that.













































% Hook into forall.fmt:
% Add proper spacing after forall-generated dots.











% We shouldn't use /=, that means not equal (even if it can be overriden)!







% XXX



%  format `stoup` = "\blackdiamond"






% Cancel the effect of \; (that is \thickspace)



% Use as in |vapply vf va (downto n) v|.
% (downto n) is parsed as an application argument, so we must undo the produced
% spacing.

% indexed big-step eval
% without environments
% big-step eval
% change big-step eval








% \, is 3mu, \! is -3mu, so this is almost \!\!.


\def\deriveDefCore{%
\begin{align*}
  \ensuremath{\Derive{\lambda (\Varid{x}\typcolon\sigma)\to \Varid{t}}} &= \ensuremath{\lambda (\Varid{x}\typcolon\sigma)\;(\Varid{dx}\typcolon\Delta \sigma)\to \Derive{\Varid{t}}} \\
  \ensuremath{\Derive{\Varid{s}\;\Varid{t}}} &= \ensuremath{\Derive{\Varid{s}}\;\Varid{t}\;\Derive{\Varid{t}}} \\
  \ensuremath{\Derive{\Varid{x}}} &= \ensuremath{\Varid{dx}} \\
  \ensuremath{\Derive{\Varid{c}}} &= \ensuremath{\DeriveConst{\Varid{c}}}
\end{align*}
}


% Drop unsightly numbers from function names. The ones at the end could be
% formatted as subscripts, but not the ones in the middle.


\part{Incremental Î»-Calculus}
\label{part:incr}

\chapter{Introduction to differentiation}
\label{sec:intro}
\label{ch:static-diff-intro}

Incremental computation (or incrementalization) has a long-standing history in
computer science~\citep{Ramalingam93}.
Often, a program needs to update quickly the output of some nontrivial function $f$
when the input to the computation changes. In this scenario, we assume we have
computed \ensuremath{\Varid{y}_{1}\mathrel{=}\Varid{f}\;\Varid{x}_{1}} and we need to compute \ensuremath{\Varid{y}_{2}} that equals \ensuremath{\Varid{f}\;\Varid{x}_{2}}.
In this scenario, programmers typically have to choose between a few undesirable
options.
\begin{itemize}
\item Programmers can call again function \ensuremath{\Varid{f}} on the updated input \ensuremath{\Varid{x}_{2}} and
  repeat the computation from scratch. This choice guarantees
  correct results and is easy to implement, but typically wastes
  computation time. Often, if the updated input is close to the
  original input, the same result can be computed much faster.
\item Programmers can write by hand a new function \ensuremath{\Varid{df}} that updates the
  output based on input changes, using various techniques.
  Running a hand-written function \ensuremath{\Varid{df}} can be much more efficient than rerunning
  \ensuremath{\Varid{f}}, but writing \ensuremath{\Varid{df}} requires significant developer effort, is
  error-prone, and requires updating \ensuremath{\Varid{df}} by hand to keep it consistent with \ensuremath{\Varid{f}}
  whenever \ensuremath{\Varid{f}} is modified. In practice, this complicates code maintenance
  significantly~\citep{Salvaneschi13reactive}.
\item Programmers can write \ensuremath{\Varid{f}} using domain-specific languages that
  support incrementalization, for tasks where such languages are
  available. For instance, build scripts (our \ensuremath{\Varid{f}}) are written in
  domain-specific languages that support (coarse-grained)
  incremental builds. Database query languages also have often
  support for incrementalization.\pg{Mention here limits?}
\item Programmers can attempt using general-purpose techniques for
  incrementalizing programs, such as \emph{self-adjusting
    computation} and variants such as \emph{Adapton}. Self-adjusting
  computation applies to arbitrary purely functional programs and
  has been extended to imperative programs; however, it only
  guarantees efficient incrementalization when applied to base
  programs that are \emph{designed} for efficient
  incrementalization.\pg{Citations}
  Nevertheless, self-adjusting computation enabled incrementalizing programs
  that had never been incrementalized by hand before.
\end{itemize}


\pg{Resume and readd this text.}
% To understand how to compute |f| incrementally, we can summarize the key idea
% behing many incrementalization approaches.\pg{self-adjusting computation.}\pg{?}
% Let us assume, for simplicity, our function |f| is written in a purely
% functional language. During a computation such as |y = f x1|, each computation
% step produce an output using some inputs. The new output can in turn be used as
% input by further steps. We can record these computation steps as a directed
% acyclic graph (DAG) representing dependencies: each node is either an initial
% input or the output of some computation steps, and each output node has incoming
% edges from all

\pg{Continue discussing dependencies minimization and the
  relation with parallelism. Build scripts might be a good
  example.}

No approach guarantees automatic efficient incrementalization for arbitrary
programs.
We propose instead to design domain-specific languages
(DSLs) that can be efficiently incrementalized, that we call \emph{incremental}
DSLs (IDSLs).

To incrementalize IDSL programs, we use a transformation that we call
\emph{(finite) differentiation}.
Differentiation produces programs in the same language, called derivatives,
that can be optimized further and compiled to efficient code.
Derivatives represent changes to values through further values, that we call
simply changes.

For primitives, IDSL designers must specify the result of
differentiation: IDSL designers are to choose primitives that encapsulate
efficiently incrementalizable computation schemes, while IDSL users are to
express their computation using the primitives provided by the IDSL\@.

Helping IDSL designers to incrementalize primitives automatically is a
desirable goal, though one that we leave open. In our setting, incrementalizing
primitives becomes a problem of \emph{program synthesis}, and we agree with
\citet{Shah2017synthesis} that it should be treated as such. Among others,
\citet{Liu00} develops a systematic approach to this synthesis problem for
first-order programs based on equational reasoning, but it is unclear how
scalable this approach is. We provide foundations for using equational
reasoning, and sketch an IDSL for handling different sorts of collections. We
also discuss avenues at providing language plugins for more fundamental
primitives, such as algebraic datatypes with structural recursion.

\pg{rewrite}
In the IDSLs we consider, similarly to database languages, we use primitives for
high-level operations, of complexity similar to SQL operators.
On the one hand, IDSL designers wish to design few general primitives to limit
the effort needed for manual incrementalization.
On the other hand, overly general primitives can be harder to incrementalize
efficiently. Nevertheless, we also provide some support for more general
building blocks such as product or sum types and even (in some situations)
recursive types.
Other approaches provide more support for incrementalizing primitives, but even
then ensuring efficient incrementalization is not necessarily easy. Dynamic
approaches to incrementalization are most powerful: they can find work that can
be reused at runtime using memoization, as long as the computation is structured
so that memoization matches will occur. Moreover, for some programs it seems
more efficient to detect that some output can be reused thanks to a description
of the input changes, rather than through runtime detection.
\pg{Consider list insertion and map. This point might need to be moved to later.}
%There is a tension between the efficiency
%We return to this point in \cref{sec:fw-primitives}.
\pg{Nevertheless, we do not compete with SAC and Adapton?}

We propose that IDSLs be higher-order, so that primitives can be parameterized
over functions and hence highly flexible, and purely functional, to enable more
powerful optimizations both before and after differentiation.
Hence, an incremental DSL is a higher-order purely functional language, composed
of a $\lambda$-calculus core extended with base types and primitives.
Various database query languages support forms of finite differentiation (see
\cref{sec:finite-diff}), but only over first-order languages, which provide only
restricted forms of operators such as \ensuremath{\Varid{map}}, \ensuremath{\Varid{filter}} or aggregation.

To support higher-order IDSLs, we define the first form of differentiation that
supports higher-order functional languages; to this end, we introduce the
concept of function changes, which contain changes to either the code
of a function value or the values it closes over. While higher-order programs can be
transformed to first-order ones, incrementalizing resulting programs is still
beyond reach for previous approaches to differentiation (see
\cref{sec:finite-diff} for earlier work and \cref{sec:rw-partial-differentials}
for later approaches).
In \cref{ch:cts,ch:defunc-fun-changes} we transform higher-order programs to first-order ones by
closure conversion or defunctionalization, but we incrementalize defunctionalized programs using
similar ideas, including changes to (defunctionalized) functions.

% Instead of extending differentiation to higher-order programs, it might be
% possible to transform higher-order programs to first-order ones and try to apply
% differentiation to the result, but as
% However, efficient handling of the generated
% algebraic data types, including the ones generated by remains currently
% However, it does not appear that the resulting
% first-order programs are particularly simpler to handle.

% While there are multiple ways to transform higher-order programs to first-order
% programs where functions are represented as data, it is useful
% \pg{why not
%   defunctionalize/closure convert/...?}

\pg{actually argue for higher-order collection DSLs over relational databases;
  we didn't do that in the cited thesis section.}
%
Our primary example will be DSLs for operations on collections:
as discussed earlier (\cref{sec:aosd13-intro}), we favor
higher-order collection DSLs over relational databases.
% moving data to relational databases requires transforming them to
% a rather different metamodel
%
\pg{ What I'm saying... should also hopefully apply to data
  layout transformations used in databases.
Flattening? Nested data? Sharding?
  And data layout transformations *are*
  available in LMS... but not in GPLs?}
% We will discuss later why we favor this
% approach.\pg{where?}

We build our incremental DSLs based on
simply-typed $\lambda$-calculus (STLC), extended with
\emph{language plugins} to define the domain-specific parts, as discussed in
\cref{sec:intro-stlc} and summarized in \cref{fig:lambda-calc}. We call our
approach \emph{ILC} for \emph{Incremental Lambda Calculus}.

The rest of this chapter is organized as follows.
In \cref{sec:generalize-fin-diff} we explain that
differentiation generalizes the calculus of finite differences, a relative of
differential calculus.
In \cref{sec:motiv-example} we show a motivating example for
our approach.
In \cref{sec:change-intro} we introduce informally
the concept of \emph{changes} as values, and in \cref{sec:higher-order-intro} we
introduce \emph{changes to functions}.
In \cref{sec:informal-derive} we define differentiation and motivate it
informally.
In \cref{sec:derive-example} we apply differentiation to our motivating example.

Correctness of ILC is far from obvious. In
\cref{ch:derive-formally,ch:change-theory}, we introduce a formal theory of
changes, and we use it to formalize differentiation and prove it correct.
\pg{check later this TOC}

\section{Generalizing the calculus of finite differences}
\label{sec:generalize-fin-diff}
% Revise terminology.
Our theory of changes generalizes an existing field of mathematics called
the \emph{calculus of finite difference}: If \ensuremath{\Varid{f}} is a real
function, one can define its \emph{finite difference}, that is a
function \ensuremath{\Delta f} such that \ensuremath{\Delta f\;\Varid{a}\;\Varid{da}\mathrel{=}\Varid{f}\;(\Varid{a}\mathbin{+}\Varid{da})\mathbin{-}\Varid{f}\;\Varid{a}}. Readers
might notice the similarity (and the differences) between the
finite difference and the derivative of \ensuremath{\Varid{f}}, since the latter is
defined as
\[f'(a) = \lim_{\Varid{da} \to 0} \frac{f (a + \Varid{da}) - f(a)}{\Varid{da}}.\]

The calculus of finite differences helps computing a closed
formula for \ensuremath{\Delta f} given a closed formula for \ensuremath{\Varid{f}}. For instance,
if function \ensuremath{\Varid{f}} is defined by \ensuremath{\Varid{f}\;\Varid{x}\mathrel{=}\mathrm{2}\cdot\Varid{x}}, one can prove
its finite difference is \ensuremath{\Delta f\;\Varid{x}\;\Varid{dx}\mathrel{=}\mathrm{2}\cdot(\Varid{x}\mathbin{+}\Varid{dx})\mathbin{-}\mathrm{2}\cdot\Varid{x}\mathrel{=}\mathrm{2}\cdot\Varid{dx}}.

Finite differences are helpful for incrementalization because
they allow computing functions on updated inputs based on results
on base inputs, if we know how inputs change. Take again for
instance \ensuremath{\Varid{f}\;\Varid{x}\mathrel{=}\mathrm{2}\cdot\Varid{x}}: if \ensuremath{\Varid{x}} is a base input and \ensuremath{\Varid{x}\mathbin{+}\Varid{dx}}
is an updated input, we can compute \ensuremath{\Varid{f}\;(\Varid{x}\mathbin{+}\Varid{dx})\mathrel{=}\Varid{f}\;\Varid{x}\mathbin{+}\Delta f\;\Varid{x}\;\Varid{dx}}. If we already computed \ensuremath{\Varid{y}\mathrel{=}\Varid{f}\;\Varid{x}} and reuse the result, we
can compute \ensuremath{\Varid{f}\;(\Varid{x}\mathbin{+}\Varid{dx})\mathrel{=}\Varid{y}\mathbin{+}\Delta f\;\Varid{x}}. Here, the input change is
\ensuremath{\Varid{dx}} and the output change is \ensuremath{\Delta f\;\Varid{x}\;\Varid{dx}}.

However, the calculus of finite differences is usually defined
for real functions. Since it is based on operators \ensuremath{\mathbin{+}} and \ensuremath{\mathbin{-}},
it can be directly extended to commutative groups.
Incrementalization based on finite differences for groups and
first-order programs has already been
researched~\citep{Paige82FDC,GlucheGrust97Incr}, most recently and
spectacularly with DBToaster~\citep{Koch10IQE,Koch2016incremental}.

But it is not immediate how to generalize finite differencing
beyond groups. And many useful types do not form a group: for
instance, lists of integers don't form a group but only a monoid.
Moreover, it's hard to represent list changes simply through a
list: how do we specify which elements were inserted (and where),
which were removed and which were subjected to change themselves?

In ILC, we generalize the calculus of finite differences by
using distinct types for base values and changes, and adapting
the surrounding theory. ILC generalizes operators \ensuremath{\mathbin{+}} and \ensuremath{\mathbin{-}} as operators
\ensuremath{\oplus } (pronounced ``oplus'' or ``update'') and \ensuremath{\ominus } (pronounced
``ominus'' or ``difference''). We show how ILC subsumes groups in
\cref{sec:change-structure-groups}.

\section{A motivating example}
\label{sec:motiv-example}
In this section, we illustrate informally incrementalization on a
small example.

In the following program, \ensuremath{\Varid{grandTotal}\;\Varid{xs}\;\Varid{ys}} sums integer numbers in
input collections \ensuremath{\Varid{xs}} and \ensuremath{\Varid{ys}}.

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{22}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{grandTotal}{}\<[22]%
\>[22]{}\mathrel{:\mkern-1mu:}\Conid{Bag}\;\mathbb{Z}\to \Conid{Bag}\;\mathbb{Z}\to \mathbb{Z}{}\<[E]%
\\
\>[3]{}\Varid{s}{}\<[22]%
\>[22]{}\mathrel{:\mkern-1mu:}\mathbb{Z}{}\<[E]%
\\[\blanklineskip]%
\>[3]{}\Varid{grandTotal}\;\Varid{xs}\;\Varid{ys}{}\<[22]%
\>[22]{}\mathrel{=}\Varid{sum}\;(\Varid{merge}\;\Varid{xs}\;\Varid{ys}){}\<[E]%
\\
\>[3]{}\Varid{s}{}\<[22]%
\>[22]{}\mathrel{=}\Varid{grandTotal}\;\Varid{xs}\;\Varid{ys}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks

This program computes output \ensuremath{\Varid{s}} from input collections \ensuremath{\Varid{xs}} and
\ensuremath{\Varid{ys}}. These collections are multisets or \emph{bags}, that is,
collections that are unordered (like sets) where elements are
allowed to appear more than once (unlike sets). In this example,
we assume a language plugin that supports a base type of integers
\ensuremath{\mathbb{Z}} and a family of base types of bags \ensuremath{\Conid{Bag}\;\tau} for any type
\ensuremath{\tau}.

We can run this program on specific inputs \ensuremath{\Varid{xs}_{1}\mathrel{=}\{\mskip1.5mu \{\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu\}\mskip1.5mu\}}
and \ensuremath{\Varid{ys}_{1}\mathrel{=}\{\mskip1.5mu \{\mskip1.5mu \mathrm{4}\mskip1.5mu\}\mskip1.5mu\}} to obtain output \ensuremath{\Varid{s}_{1}}. Here, double braces
\ensuremath{\{\mskip1.5mu \{\mskip1.5mu \ldots\mskip1.5mu\}\mskip1.5mu\}} denote a bag containing the elements among the double
braces.

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{22}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{s}_{1}{}\<[22]%
\>[22]{}\mathrel{=}\Varid{grandTotal}\;\Varid{xs}_{1}\;\Varid{ys}_{1}{}\<[E]%
\\
\>[22]{}\mathrel{=}\Varid{sum}\;\{\mskip1.5mu \{\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4}\mskip1.5mu\}\mskip1.5mu\}\mathrel{=}\mathrm{10}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks

This example uses small inputs for simplicity, but in practice they
are typically much bigger; we use \ensuremath{\Varid{n}}
to denote the input size. In this case the asymptotic complexity of computing
\ensuremath{\Varid{s}} is \ensuremath{\Theta(\Varid{n})}.

Consider computing updated output \ensuremath{\Varid{s}_{2}} from updated inputs \ensuremath{\Varid{xs}_{2}\mathrel{=}\{\mskip1.5mu \{\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu\}\mskip1.5mu\}}
and \ensuremath{\Varid{ys}_{2}\mathrel{=}\{\mskip1.5mu \{\mskip1.5mu \mathrm{4},\mathrm{5}\mskip1.5mu\}\mskip1.5mu\}}. We could recompute \ensuremath{\Varid{s}_{2}} from scratch as
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{s}_{2}{}\<[16]%
\>[16]{}\mathrel{=}\Varid{grandTotal}\;\Varid{xs}_{2}\;\Varid{ys}_{2}{}\<[E]%
\\
\>[16]{}\mathrel{=}\Varid{sum}\;\{\mskip1.5mu \{\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{2},\mathrm{3},\mathrm{4},\mathrm{5}\mskip1.5mu\}\mskip1.5mu\}\mathrel{=}\mathrm{16}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
But if the size of the updated inputs is \ensuremath{\Theta(\Varid{n})}, recomputation also
takes time \ensuremath{\Theta(\Varid{n})}, and we would like to obtain our result asymptotically faster.

To compute the updated output \ensuremath{\Varid{s}_{2}} faster, we assume the changes to the
inputs have a description of size \ensuremath{\Varid{dn}} that is asymptotically smaller than the
input size \ensuremath{\Varid{n}}, that is \ensuremath{\Varid{dn}\mathrel{=}o(\Varid{n})}. All approaches to incrementalization
require small input changes. Incremental computation will then process the input
changes, rather than just the new inputs.

\section{Introducing changes}
\label{sec:change-intro}
To talk about how the differences between old values and new
values, we introduce a few concepts, for now without full definitions.
In our approach to
incrementalization, we describe changes to values as values
themselves: We call such descriptions simply \emph{changes}.
Incremental programs examine changes to inputs to understand how to produce
changes to outputs. Just
like in STLC we have terms (programs) that evaluates to values,
we also have \emph{change terms}, which evaluate to \emph{change
  values}. We require that going from old values to new values
preserves types: That is, if an old value \ensuremath{\Varid{v}_{1}} has type \ensuremath{\tau},
then also its corresponding new value \ensuremath{\Varid{v}_{2}} must have type \ensuremath{\tau}.
To each type \ensuremath{\tau} we associate a type of changes or \emph{change type}
\ensuremath{\Delta \tau}: a change between \ensuremath{\Varid{v}_{1}} and \ensuremath{\Varid{v}_{2}} must be a value of type \ensuremath{\Delta \tau}.
Similarly, environments can change: to typing context \ensuremath{\Gamma} we associate
change typing contexts \ensuremath{\Delta \Gamma}, such that we can have an environment change
\ensuremath{\D\rho\typcolon\Eval{\Delta \Gamma}} from \ensuremath{\rho_{1}\typcolon\Eval{\Gamma}} to \ensuremath{\rho_{2}\typcolon\Eval{\Gamma}}.

Not all descriptions of changes are meaningful,
so we also talk about \emph{valid} changes. Valid changes satisfy additional
invariants that are useful during incrementalization.
%
A change value \ensuremath{\Varid{dv}} can be a valid change from \ensuremath{\Varid{v}_{1}} to \ensuremath{\Varid{v}_{2}}. We
can also consider a valid change as an edge from \ensuremath{\Varid{v}_{1}} to \ensuremath{\Varid{v}_{2}} in
a graph associated to \ensuremath{\tau} (where the vertexes are values of
type \ensuremath{\tau}), and we call \ensuremath{\Varid{v}_{1}} the source of \ensuremath{\Varid{dv}} and \ensuremath{\Varid{v}_{2}} the
destination of \ensuremath{\Varid{dv}}. We only talk of source and destination for valid changes:
so a change from \ensuremath{\Varid{v}_{1}} to \ensuremath{\Varid{v}_{2}} is (implicitly) valid.
We'll discuss examples of valid and invalid
changes in \cref{ex:valid-bag-int,ex:invalid-nat}.\pg{What about
  changes with multiple valid sources?}

We also introduce an operator \ensuremath{\oplus } on values and changes: if
\ensuremath{\Varid{dv}} is a valid change from \ensuremath{\Varid{v}_{1}} to \ensuremath{\Varid{v}_{2}}, then \ensuremath{\Varid{v}_{1}\oplus \Varid{dv}}
(read as ``\ensuremath{\Varid{v}_{1}} updated by \ensuremath{\Varid{dv}}'') is guaranteed to return \ensuremath{\Varid{v}_{2}}.
If \ensuremath{\Varid{dv}} is \emph{not} a valid change from \ensuremath{\Varid{v}_{1}}, then \ensuremath{\Varid{v}_{1}\oplus \Varid{dv}} can be
defined to some arbitrary value or not, without any effect on correctness. In
practice, if \ensuremath{\oplus } detects an invalid input change it can trigger an error or
return a dummy value; in our formalization we assume for simplicity that
\ensuremath{\oplus } is total.
Again, if \ensuremath{\Varid{dv}} is not valid from \ensuremath{\Varid{v}_{1}} to \ensuremath{\Varid{v}_{1}\oplus \Varid{dv}}, then we do not talk of
the source and destination of \ensuremath{\Varid{dv}}.

We also introduce operator \ensuremath{\ominus }: given two values \ensuremath{\Varid{v}_{1},\Varid{v}_{2}}
for the same type, \ensuremath{\Varid{v}_{2}\ominus \Varid{v}_{1}} is a valid change from \ensuremath{\Varid{v}_{1}}
to \ensuremath{\Varid{v}_{2}}.

Finally, we introduce change composition: if \ensuremath{\Varid{dv}_{1}} is a valid
change from \ensuremath{\Varid{v}_{1}} to \ensuremath{\Varid{v}_{2}} and \ensuremath{\Varid{dv}_{2}} is a valid change from \ensuremath{\Varid{v}_{2}} to
\ensuremath{\Varid{v}_{3}}, then \ensuremath{\Varid{dv}_{1}\circledcirc\Varid{dv}_{2}} is a valid change from \ensuremath{\Varid{v}_{1}} to
\ensuremath{\Varid{v}_{3}}.

Change operators are overloaded over different types.
Coherent definitions of validity and of operators \ensuremath{\oplus ,\ominus } and
\ensuremath{\circledcirc } for a type \ensuremath{\tau} form a \emph{change structure} over values of type
\ensuremath{\tau} (\cref{def:change-structure}).
For each type \ensuremath{\tau} we'll define a change structure (\cref{def:chs-types}),
and operators will have types \ensuremath{\oplus \typcolon\tau\to \Delta \tau\to \tau}, \ensuremath{\ominus \typcolon\tau\to \tau\to \Delta \tau}, \ensuremath{\circledcirc \typcolon\Delta \tau\to \Delta \tau\to \Delta \tau}.

\begin{example}[Changes on integers and bags]
  \label{ex:valid-bag-int}
  \label{ex:chs-int}
 \pg{changes on bags?}
To show how incrementalization affects our example, we next
describe valid changes for integers and bags.
%
For now, a change
\ensuremath{\Varid{das}} to a bag \ensuremath{\Varid{as}_{1}} simply contains all elements to be added to
the initial bag \ensuremath{\Varid{as}_{1}} to obtain the updated bag \ensuremath{\Varid{as}_{2}} (we'll ignore
removing elements for this section and discuss it later). In our
example, the change from \ensuremath{\Varid{xs}_{1}} (that is \ensuremath{\{\mskip1.5mu \{\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu\}\mskip1.5mu\}}) to \ensuremath{\Varid{xs}_{2}}
(that is \ensuremath{\{\mskip1.5mu \{\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu\}\mskip1.5mu\}}) is \ensuremath{\Varid{dxs}\mathrel{=}\{\mskip1.5mu \{\mskip1.5mu \mathrm{1}\mskip1.5mu\}\mskip1.5mu\}}, while the change
from \ensuremath{\Varid{ys}_{1}} (that is \ensuremath{\{\mskip1.5mu \{\mskip1.5mu \mathrm{4}\mskip1.5mu\}\mskip1.5mu\}}) to \ensuremath{\Varid{ys}_{2}} (that is \ensuremath{\{\mskip1.5mu \{\mskip1.5mu \mathrm{4},\mathrm{5}\mskip1.5mu\}\mskip1.5mu\}}) is
\ensuremath{\Varid{dys}\mathrel{=}\{\mskip1.5mu \{\mskip1.5mu \mathrm{5}\mskip1.5mu\}\mskip1.5mu\}}.
%
To represent the output change \ensuremath{\Varid{ds}} from
\ensuremath{\Varid{s}_{1}} to \ensuremath{\Varid{s}_{2}} we need integer changes. For now, we
represent integer changes as integers, and define \ensuremath{\oplus } on
integers as addition: \ensuremath{\Varid{v}_{1}\oplus \Varid{dv}\mathrel{=}\Varid{v}_{1}\mathbin{+}\Varid{dv}}.
\end{example}

For both bags and integers, a change \ensuremath{\Varid{dv}} is always valid between
\ensuremath{\Varid{v}_{1}} and \ensuremath{\Varid{v}_{2}\mathrel{=}\Varid{v}_{1}\oplus \Varid{dv}}; for other changes, however,
validity will be more restrictive.
\begin{example}[Changes on naturals]
  \label{ex:invalid-nat}
For instance, say we want to
define changes on a type of natural numbers, and we still want to
have \ensuremath{\Varid{v}_{1}\oplus \Varid{dv}\mathrel{=}\Varid{v}_{1}\mathbin{+}\Varid{dv}}. A change from \ensuremath{\mathrm{3}} to \ensuremath{\mathrm{2}} should
still be \ensuremath{\mathbin{-}\mathrm{1}}, so the type of changes must be \ensuremath{\mathbb{Z}}. But the
result of \ensuremath{\oplus } should still be a natural, that is an integer
\ensuremath{\geq \mathrm{0}}: to ensure that \ensuremath{\Varid{v}_{1}\oplus \Varid{dv}\geq \mathrm{0}} we need to require
that \ensuremath{\Varid{dv}\geq \mathbin{-}\Varid{v}_{1}}. We use this requirement to define validity on
naturals: \ensuremath{\validfromto{\mathbb{N}}{\Varid{v}_{1}}{\Varid{dv}}{\Varid{v}_{1}\mathbin{+}\Varid{dv}}} is defined as equivalent to
\ensuremath{\Varid{dv}\geq \mathbin{-}\Varid{v}_{1}}. We can guarantee equation \ensuremath{\Varid{v}_{1}\oplus \Varid{dv}\mathrel{=}\Varid{v}_{1}\mathbin{+}\Varid{dv}}
not for all changes, but only for valid changes. Conversely, if a
change \ensuremath{\Varid{dv}} is invalid for \ensuremath{\Varid{v}_{1}}, then \ensuremath{\Varid{v}_{1}\mathbin{+}\Varid{dv}\mathbin{<}\mathrm{0}}. We then
define \ensuremath{\Varid{v}_{1}\oplus \Varid{dv}} to be \ensuremath{\mathrm{0}}, though any other definition on
invalid changes would work.\footnote{In fact, we could leave
  \ensuremath{\oplus } undefined on invalid changes. Our original
  presentation~\citep{CaiEtAl2014ILC}, in essence, restricted
  \ensuremath{\oplus } to valid changes through dependent types, by ensuring
  that applying it to invalid changes would be ill-typed. Later,
  \citet{Huesca2015incrementality}, in similar developments,
  simply made \ensuremath{\oplus } partial on its domain instead of
  restricting the domain, achieving similar results.}
\end{example}
\pg{bags with removals? where?}

\subsection{Incrementalizing with changes}
After introducing changes and related notions, we describe how we incrementalize
our example program.

We consider again the scenario of \cref{sec:motiv-example}: we need to compute
the updated output \ensuremath{\Varid{s}_{2}}, the result of calling our program \ensuremath{\Varid{grandTotal}} on
updated inputs \ensuremath{\Varid{xs}_{2}} and \ensuremath{\Varid{ys}_{2}}. And we have the initial output \ensuremath{\Varid{s}_{1}} from calling our
program on initial inputs \ensuremath{\Varid{xs}_{1}} and \ensuremath{\Varid{ys}_{1}}. In this scenario we can compute \ensuremath{\Varid{s}_{2}}
\emph{non-incrementally} by calling \ensuremath{\Varid{grandTotal}} on the updated inputs, but
we would like to obtain the same result faster.
Hence, we compute \ensuremath{\Varid{s}_{2}} \emph{incrementally}: that is, we first compute the
\emph{output change} \ensuremath{\Varid{ds}} from \ensuremath{\Varid{s}_{1}} to \ensuremath{\Varid{s}_{2}}; then we update the old output \ensuremath{\Varid{s}_{1}}
by change \ensuremath{\Varid{ds}}. Successful incremental computation must compute the correct \ensuremath{\Varid{s}_{2}}
asymptotically faster than non-incremental computation. This speedup is possible
because we take advantage of the computation already done to compute \ensuremath{\Varid{s}_{1}}.

To compute the output change \ensuremath{\Varid{ds}} from \ensuremath{\Varid{s}_{1}} to \ensuremath{\Varid{s}_{2}}, we propose to transform our
\emph{base program} \ensuremath{\Varid{grandTotal}} to a new program \ensuremath{\Varid{dgrandTotal}}, that we call
the \emph{derivative} of \ensuremath{\Varid{grandTotal}}: to compute \ensuremath{\Varid{ds}} we call \ensuremath{\Varid{dgrandTotal}}
on initial inputs and their respective changes.
Unlike other approaches to incrementalization, \ensuremath{\Varid{dgrandTotal}} is a regular
program in the same language as \ensuremath{\Varid{grandTotal}}, hence can be further optimized
with existing technology.

Below, we give the code for \ensuremath{\Varid{dgrandTotal}} and show that in this example
incremental computation computes \ensuremath{\Varid{s}_{2}} correctly.

For ease of reference, we recall inputs, changes and outputs:
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{31}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{xs}_{1}{}\<[31]%
\>[31]{}\mathrel{=}\{\mskip1.5mu \{\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu\}\mskip1.5mu\}{}\<[E]%
\\
\>[3]{}\Varid{dxs}{}\<[31]%
\>[31]{}\mathrel{=}\{\mskip1.5mu \{\mskip1.5mu \mathrm{1}\mskip1.5mu\}\mskip1.5mu\}{}\<[E]%
\\
\>[3]{}\Varid{xs}_{2}{}\<[31]%
\>[31]{}\mathrel{=}\{\mskip1.5mu \{\mskip1.5mu \mathrm{1},\mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu\}\mskip1.5mu\}{}\<[E]%
\\
\>[3]{}\Varid{ys}_{1}{}\<[31]%
\>[31]{}\mathrel{=}\{\mskip1.5mu \{\mskip1.5mu \mathrm{4}\mskip1.5mu\}\mskip1.5mu\}{}\<[E]%
\\
\>[3]{}\Varid{dys}{}\<[31]%
\>[31]{}\mathrel{=}\{\mskip1.5mu \{\mskip1.5mu \mathrm{5}\mskip1.5mu\}\mskip1.5mu\}{}\<[E]%
\\
\>[3]{}\Varid{ys}_{2}{}\<[31]%
\>[31]{}\mathrel{=}\{\mskip1.5mu \{\mskip1.5mu \mathrm{4},\mathrm{5}\mskip1.5mu\}\mskip1.5mu\}{}\<[E]%
\\
\>[3]{}\Varid{s}_{1}{}\<[31]%
\>[31]{}\mathrel{=}\Varid{grandTotal}\;\Varid{xs}_{1}\;\Varid{ys}_{1}{}\<[E]%
\\
\>[31]{}\mathrel{=}\mathrm{10}{}\<[E]%
\\
\>[3]{}\Varid{s}_{2}{}\<[31]%
\>[31]{}\mathrel{=}\Varid{grandTotal}\;\Varid{xs}_{2}\;\Varid{ys}_{2}{}\<[E]%
\\
\>[31]{}\mathrel{=}\mathrm{16}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
Incremental computation uses the following definitions to compute \ensuremath{\Varid{s}_{2}} correctly
and with fewer steps, as desired.
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{31}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[3]{}\Varid{dgrandTotal}\;\Varid{xs}\;\Varid{dxs}\;\Varid{ys}\;\Varid{dys}{}\<[31]%
\>[31]{}\mathrel{=}\Varid{sum}\;(\Varid{merge}\;\Varid{dxs}\;\Varid{dys}){}\<[E]%
\\
\>[3]{}\Varid{ds}{}\<[31]%
\>[31]{}\mathrel{=}\Varid{dgrandTotal}\;\Varid{xs}_{1}\;\Varid{dxs}\;\Varid{ys}_{1}\;\Varid{dys}\mathrel{=}{}\<[E]%
\\
\>[31]{}\mathrel{=}\Varid{sum}\;\{\mskip1.5mu \{\mskip1.5mu \mathrm{1},\mathrm{5}\mskip1.5mu\}\mskip1.5mu\}\mathrel{=}\mathrm{6}{}\<[E]%
\\
\>[3]{}\Varid{s}_{2}{}\<[31]%
\>[31]{}\mathrel{=}\Varid{s}_{1}\oplus \Varid{ds}\mathrel{=}\Varid{s}_{1}\mathbin{+}\Varid{ds}{}\<[E]%
\\
\>[31]{}\mathrel{=}\mathrm{10}\mathbin{+}\mathrm{6}\mathrel{=}\mathrm{16}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks

Incremental computation should be asymptotically faster than non-incremental
computation; hence, the derivative we run should be asymptotically faster than
the base program.
Here, derivative \ensuremath{\Varid{dgrandTotal}} is faster simply because it \emph{ignores} initial
inputs altogether. Therefore, its time complexity depends only on the total size
of changes \ensuremath{\Varid{dn}}. In particular, the complexity of \ensuremath{\Varid{dgrandTotal}} is \ensuremath{\Theta(\Varid{dn})\mathrel{=}o(\Varid{n})}.

We generate derivatives through a program transformation from terms to terms,
which we call \emph{differentiation} (or, sometimes, simply \emph{derivation}).
We write \ensuremath{\Derive{\Varid{t}}} for the result of
differentiating term \ensuremath{\Varid{t}}. We apply \ensuremath{\Derive{\text{\textendash}}} on terms of our non-incremental
programs or \emph{base terms}, such as \ensuremath{\Varid{grandTotal}}. To define differentiation,
we assume that we already have derivatives for primitive functions they use; we
discuss later how to write such derivatives by hand.

We define differentiation in \cref{def:derive}; some readers might prefer to
peek ahead, but we prefer to first explain what differentiation is supposed to do.

A derivative of a function can be applied to initial inputs and changes from initial
inputs to updated inputs, and returns a change from an initial output to an
updated output. For instance, take derivative \ensuremath{\Varid{dgrandTotal}}, initial inputs
\ensuremath{\Varid{xs}_{1}} and \ensuremath{\Varid{ys}_{1}}, and changes \ensuremath{\Varid{dxs}} and \ensuremath{\Varid{dys}} from initial inputs to updated
inputs. Then, change \ensuremath{\Varid{dgrandTotal}\;\Varid{xs}_{1}\;\Varid{dxs}\;\Varid{ys}_{1}\;\Varid{dys}}, that is \ensuremath{\Varid{ds}}, goes from initial
output \ensuremath{\Varid{grandTotal}\;\Varid{xs}_{1}\;\Varid{ys}_{1}}, that is \ensuremath{\Varid{s}_{1}}, to updated output \ensuremath{\Varid{grandTotal}\;\Varid{xs}_{2}\;\Varid{ys}_{2}}, that is \ensuremath{\Varid{s}_{2}}. And because \ensuremath{\Varid{ds}} goes from \ensuremath{\Varid{s}_{1}} to \ensuremath{\Varid{s}_{2}}, it follows as a
corollary that \ensuremath{\Varid{s}_{2}\mathrel{=}\Varid{s}_{1}\oplus \Varid{ds}}. Hence, we can compute \ensuremath{\Varid{s}_{2}} incrementally
through \ensuremath{\Varid{s}_{1}\oplus \Varid{ds}}, as we have shown, rather than by evaluating
\ensuremath{\Varid{grandTotal}\;\Varid{xs}_{2}\;\Varid{ys}_{2}}.

We often just say that a derivative
of function \ensuremath{\Varid{f}} maps changes to the inputs of \ensuremath{\Varid{f}} to changes to the outputs of
\ensuremath{\Varid{f}}, leaving the initial inputs implicit. In short:
\begin{restatable}{slogan}{sloganDerive}
  \label{slogan:derive}
  \emph{Term \ensuremath{\Derive{\Varid{t}}} maps input changes to output changes.}
  That is, \ensuremath{\Derive{\Varid{t}}} applied to initial base inputs and valid \emph{input changes}
  (from initial inputs to updated inputs) gives a valid \emph{output change} from \ensuremath{\Varid{t}}
  applied on old inputs to \ensuremath{\Varid{t}} applied on new inputs.
\end{restatable}

For a
generic unary function \ensuremath{\Varid{f}\typcolon\Conid{A}\to \Conid{B}}, the behavior of \ensuremath{\Derive{\Varid{f}}} can be described as:
\begin{equation}
  % \label{eq:derivative-requirement}
  \label{eq:correctness}
  \ensuremath{\Varid{f}\;\Varid{a}_{2}\cong\Varid{f}\;\Varid{a}_{1}\oplus \Derive{\Varid{f}}\;\Varid{a}_{1}\;\Varid{da}}
\end{equation}
or as
\begin{equation}
  % \label{eq:derivative-requirement}
  \label{eq:correctness-alt}
  \ensuremath{\Varid{f}\;(\Varid{a}_{1}\oplus \Varid{da})\cong\Varid{f}\;\Varid{a}_{1}\oplus \Derive{\Varid{f}}\;\Varid{a}_{1}\;\Varid{da}}
\end{equation}
where \ensuremath{\Varid{da}} is a metavariable standing for a valid change from \ensuremath{\Varid{a}_{1}} to \ensuremath{\Varid{a}_{2}} (with \ensuremath{\Varid{a}_{1},\Varid{a}_{2}\typcolon\Conid{A}}) and
where \ensuremath{\cong} denotes denotational equivalence (\cref{def:denot-equivalence}).
Moreover, \ensuremath{\Derive{\Varid{f}}\;\Varid{a}_{1}\;\Varid{da}} is also a valid change and can be hence used as an
argument for operations that require valid changes.
These equations follow from \cref{thm:derive-correct} and
\cref{thm:derive-correct-oplus}; we iron out the few remaining details to obtain
these equations in \cref{sec:denot-syntactic-reasoning}.\footnote{Nitpick: if
  \ensuremath{\Varid{da}} is read as an object variable, denotational equivalence will detect that
  these terms are not equivalent if \ensuremath{\Varid{da}} maps to an invalid change. Hence we
  said that \ensuremath{\Varid{da}} is a metavariable. Later we define denotational equivalence for valid changes
  (\cref{def:denot-equivalence-valid-changes}), which gives a less cumbersome way
  to state such equations.}%
\pg{So we still need to say ``a derivative'', not ``the derivative''.}

In our example, we have applied \ensuremath{\Derive{\text{\textendash}}} to
\ensuremath{\Varid{grandTotal}}, and simplify the result via
$\beta$-reduction to produce \ensuremath{\Varid{dgrandTotal}}, as we show in \cref{sec:derive-example-merge}.
Correctness of \ensuremath{\Derive{\text{\textendash}}} guarantees
that \ensuremath{\Varid{sum}\;(\Varid{merge}\;\Varid{dxs}}\linebreak\ensuremath{\Varid{dys})} evaluates to a change from
\ensuremath{\Varid{sum}\;(\Varid{merge}\;\Varid{xs}\;\Varid{ys})} evaluated on old inputs \ensuremath{\Varid{xs}_{1}} and \ensuremath{\Varid{ys}_{1}} to
\ensuremath{\Varid{sum}\;(\Varid{merge}}\linebreak\ensuremath{\Varid{xs}\;\Varid{ys})} evaluated on new inputs \ensuremath{\Varid{xs}_{2}} and \ensuremath{\Varid{ys}_{2}}.

In this section, we have sketched the meaning of differentiation
informally. We discuss incrementalization on higher-order
terms in \cref{sec:higher-order-intro}, and actually define
differentiation in \cref{sec:informal-derive}.


\section{Differentiation on open terms and functions}
\label{sec:higher-order-intro}
We have shown that applying \ensuremath{\Derive{\text{\textendash}}} on closed functions produces their
derivatives. However, \ensuremath{\Derive{\text{\textendash}}} is defined for all terms, hence also for open
terms and for non-function types.

% Add type annotations.
Open terms \ensuremath{\Gamma\vdash\Varid{t}\typcolon\tau} are evaluated with respect to an environment for
\ensuremath{\Gamma}, and when this environment changes, the result of \ensuremath{\Varid{t}} changes as well;
\ensuremath{\Derive{\Varid{t}}} computes the change to \ensuremath{\Varid{t}}'s output.
If \ensuremath{\Gamma\vdash\Varid{t}\typcolon\tau}, evaluating term \ensuremath{\Derive{\Varid{t}}} requires as input a
\emph{change environment} \ensuremath{\D\rho\typcolon\Eval{\Delta \Gamma}} containing changes from the \emph{initial
environment} \ensuremath{\rho_{1}\typcolon\Eval{\Gamma}} to the \emph{updated environment} \ensuremath{\rho_{2}\typcolon\Eval{\Gamma}}.
The (environment) input change \ensuremath{\D\rho} is mapped by \ensuremath{\Derive{\Varid{t}}} to output change
\ensuremath{\Varid{dv}\mathrel{=}\Eval{\Derive{\Varid{t}}}\;\D\rho}, a change from \emph{initial output} \ensuremath{\Eval{\Varid{t}}\;\rho_{1}} to \emph{updated output} \ensuremath{\Eval{\Varid{t}}\;\rho_{2}}. If \ensuremath{\Varid{t}} is a function,
\ensuremath{\Varid{dv}} maps in turn changes to the function arguments to changes to the function
result. All this behavior, again, follows our slogan.

Environment changes contains changes for each variable in \ensuremath{\Gamma}. More precisely, if
variable \ensuremath{\Varid{x}} appears with type \ensuremath{\tau} in \ensuremath{\Gamma} and hence in \ensuremath{\rho_{1},\rho_{2}}, then
\ensuremath{\Varid{dx}} appears with type \ensuremath{\Delta \tau} in \ensuremath{\Delta \Gamma} and hence in \ensuremath{\D\rho}. Moreover,
\ensuremath{\D\rho} extends \ensuremath{\rho_{1}} to provide \ensuremath{\Derive{\Varid{t}}} with initial inputs and not just
with their changes.

The two environments
\ensuremath{\rho_{1}} and \ensuremath{\rho_{2}} can share entries---in particular, environment change \ensuremath{\D\rho} can
be a \emph{nil change} from \ensuremath{\rho} to \ensuremath{\rho}. For instance, \ensuremath{\Gamma} can be empty:
then \ensuremath{\rho_{1}} and \ensuremath{\rho_{2}} are also empty (since they match \ensuremath{\Gamma}) and equal, so
\ensuremath{\D\rho} is a nil change. Alternatively, some or all the change entries in \ensuremath{\D\rho}
can be nil changes. Whenever \ensuremath{\D\rho} is a nil change, \ensuremath{\Eval{\Derive{\Varid{t}}}\;\D\rho} is
also a nil change.

If \ensuremath{\Varid{t}} is a function, \ensuremath{\Derive{\Varid{t}}} will be a \emph{function change}.
Changes to functions in turn map input changes to output changes, following our
\cref{slogan:derive}. If a change \ensuremath{\Varid{df}} from \ensuremath{\Varid{f}_{1}} to \ensuremath{\Varid{f}_{2}} is applied (via \ensuremath{\Varid{df}\;\Varid{a}_{1}\;\Varid{da}}) to an input change \ensuremath{\Varid{da}} from \ensuremath{\Varid{a}_{1}} to \ensuremath{\Varid{a}_{2}}, then \ensuremath{\Varid{df}} will produce a change
\ensuremath{\Varid{dv}\mathrel{=}\Varid{df}\;\Varid{a}_{1}\;\Varid{da}} from \ensuremath{\Varid{v}_{1}\mathrel{=}\Varid{f}_{1}\;\Varid{a}_{1}} to \ensuremath{\Varid{v}_{2}\mathrel{=}\Varid{f}_{2}\;\Varid{a}_{2}}. The definition of function
changes is recursive on types: that is, \ensuremath{\Varid{dv}} can in turn be a function change
mapping input changes to output changes.

Derivatives are a special case of function changes: a derivative \ensuremath{\Varid{df}} is simply
a change from \ensuremath{\Varid{f}} to \ensuremath{\Varid{f}} itself, which maps input changes \ensuremath{\Varid{da}} from \ensuremath{\Varid{a}_{1}} to \ensuremath{\Varid{a}_{2}}
to output changes \ensuremath{\Varid{dv}\mathrel{=}\Varid{df}\;\Varid{a}_{1}\;\Varid{da}} from \ensuremath{\Varid{f}\;\Varid{a}_{1}} to \ensuremath{\Varid{f}\;\Varid{a}_{2}}. This definition
coincides with the earlier definition of derivatives, and it also
coincides with the definition of function changes for the special case where \ensuremath{\Varid{f}_{1}\mathrel{=}\Varid{f}_{2}\mathrel{=}\Varid{f}}. That is why \ensuremath{\Derive{\Varid{t}}} produces derivatives if \ensuremath{\Varid{t}} is a closed
function term: we can only evaluate \ensuremath{\Derive{\Varid{t}}} against an nil environment
change, producing a nil function change.

Since the concept of function changes can be surprising, we examine it more
closely next.

\subsection{Producing function changes}

A first-class function can close over free variables that can
change, hence functions values themselves can change; hence, we
introduce \emph{function changes} to describe these changes.

% Mapping from variables to values:
% x -> a
% y -> v

For instance, term \ensuremath{t_f\mathrel{=}\lambda \Varid{x}\to \Varid{x}\mathbin{+}\Varid{y}} is a function that closes
over \ensuremath{\Varid{y}}, so different values \ensuremath{\Varid{v}} for \ensuremath{\Varid{y}} give rise to different
values for \ensuremath{\Varid{f}\mathrel{=}\Eval{t_f}\;(\Varid{y}\mathrel{=}\Varid{v})}. Take a change \ensuremath{\Varid{dv}} from \ensuremath{\Varid{v}_{1}\mathrel{=}\mathrm{5}} to \ensuremath{\Varid{v}_{2}\mathrel{=}\mathrm{6}}; different inputs \ensuremath{\Varid{v}_{1}} and \ensuremath{\Varid{v}_{2}} for \ensuremath{\Varid{y}} give
rise to different outputs \ensuremath{\Varid{f}_{1}\mathrel{=}\Eval{t_f}\;(\Varid{y}\mathrel{=}\Varid{v}_{1})} and \ensuremath{\Varid{f}_{2}\mathrel{=}\Eval{t_f}\;(\Varid{y}\mathrel{=}\Varid{v}_{2})}.
%
We describe the difference between outputs \ensuremath{\Varid{f}_{1}} and \ensuremath{\Varid{f}_{2}} through a function
change \ensuremath{\Varid{df}} from \ensuremath{\Varid{f}_{1}} to \ensuremath{\Varid{f}_{2}}.

Consider again \cref{slogan:derive} and how it applies to term
\ensuremath{\Varid{f}}:
%
\sloganDerive*
%
Since \ensuremath{\Varid{y}} is free in \ensuremath{t_f}, the value for \ensuremath{\Varid{y}} is an input of
\ensuremath{t_f}. So, continuing our example, \ensuremath{\Varid{dt}_f\mathrel{=}\Derive{t_f}} must map
a valid input change \ensuremath{\Varid{dv}} from \ensuremath{\Varid{v}_{1}} to \ensuremath{\Varid{v}_{2}} for variable \ensuremath{\Varid{y}} to
a valid output change \ensuremath{\Varid{df}} from \ensuremath{\Varid{f}_{1}} to \ensuremath{\Varid{f}_{2}}; more precisely, we must
have \ensuremath{\Varid{df}\mathrel{=}\Eval{\Varid{dt}_f}\;(\Varid{y}\mathrel{=}\Varid{v}_{1},\Varid{dy}\mathrel{=}\Varid{dv})}.

\subsection{Consuming function changes}
Function changes can not only be produced but also be consumed in
programs obtained from \ensuremath{\Derive{\text{\textendash}}}. We discuss next how.

As discussed, we consider the value for \ensuremath{\Varid{y}} as an input to \ensuremath{t_f\mathrel{=}\lambda \Varid{x}\to \Varid{x}\mathbin{+}\Varid{y}}.
%
However, we also choose to consider the argument for \ensuremath{\Varid{x}} as an
input (of a different sort) to \ensuremath{t_f\mathrel{=}\lambda \Varid{x}\to \Varid{x}\mathbin{+}\Varid{y}}, and we require
our \cref{slogan:derive} to apply to input \ensuremath{\Varid{x}} too. While this
might sound surprising, it works out well.
Specifically,
since \ensuremath{\Varid{df}\mathrel{=}\Eval{\Derive{t_f}}} is a change from \ensuremath{\Varid{f}_{1}} to \ensuremath{\Varid{f}_{2}}, we
require \ensuremath{\Varid{df}\;\Varid{a}_{1}\;\Varid{da}} to be a change from \ensuremath{\Varid{f}_{1}\;\Varid{a}_{1}} to \ensuremath{\Varid{f}_{2}\;\Varid{a}_{2}}, so
\ensuremath{\Varid{df}} maps base input \ensuremath{\Varid{a}_{1}} and input change \ensuremath{\Varid{da}} to output change
\ensuremath{\Varid{df}\;\Varid{a}_{1}\;\Varid{da}}, satisfying the slogan.

More in general, any valid function change \ensuremath{\Varid{df}} from \ensuremath{\Varid{f}_{1}} to \ensuremath{\Varid{f}_{2}}
(where \ensuremath{\Varid{f}_{1},\Varid{f}_{2}\typcolon\Eval{\sigma\to \tau}}) must in turn be a function
that takes an input \ensuremath{\Varid{a}_{1}} and a change \ensuremath{\Varid{da}}, valid from \ensuremath{\Varid{a}_{1}}
to \ensuremath{\Varid{a}_{2}}, to a valid change \ensuremath{\Varid{df}\;\Varid{a}_{1}\;\Varid{da}} from \ensuremath{\Varid{f}_{1}\;\Varid{a}_{1}} to \ensuremath{\Varid{f}_{2}\;\Varid{a}_{2}}.

This way, to satisfy our slogan on application \ensuremath{\Varid{t}\mathrel{=}t_f\;\Varid{x}}, we can
simply define \ensuremath{\Derive{\text{\textendash}}} so that \ensuremath{\Derive{t_f\;\Varid{x}}\mathrel{=}\Derive{t_f}\;\Varid{x}\;\Varid{dx}}. Then
\[\ensuremath{\Eval{\Derive{t_f\;\Varid{x}}}\;(\Varid{y}\mathrel{=}\Varid{v}_{1},\Varid{dy}\mathrel{=}\Varid{dv},\Varid{x}\mathrel{=}\Varid{a}_{1},\Varid{dx}\mathrel{=}\Varid{da})\mathrel{=}\Eval{\Derive{t_f}}\;\Varid{a}_{1}\;\Varid{da}\mathrel{=}\Varid{df}\;\Varid{a}_{1}\;\Varid{da}}.\]
As required, that's a
change from \ensuremath{\Varid{f}_{1}\;\Varid{a}_{1}} to \ensuremath{\Varid{f}_{2}\;\Varid{a}_{2}}.

\pg{Maybe this is too steep now.}
Overall, valid function changes preserve validity, just like \ensuremath{\Derive{\Varid{t}}} in
\cref{slogan:derive}, and map valid input changes to valid output changes. In
turn, output changes can be function changes; since they are valid, they in turn
map valid changes to their inputs to valid output changes (as we'll see in
\cref{lem:validity-binary-functions}). We'll later formalize this and define
validity by recursion on types, that is, as a \emph{logical relation} (see
\cref{sec:validity-logical}).

\subsection{Pointwise function changes}
\label{ssec:pointwise-changes-intro}
It might seem more natural to describe a function change \ensuremath{\Varid{df'}} between \ensuremath{\Varid{f}_{1}} and
\ensuremath{\Varid{f}_{2}} via \ensuremath{\Varid{df'}\;\Varid{x}\mathrel{=}\lambda \Varid{x}\to \Varid{f}_{2}\;\Varid{x}\ominus \Varid{f}_{1}\;\Varid{x}}. We call such a \ensuremath{\Varid{df'}} a pointwise
change.
While some might find pointwise changes a more natural concept, the output of
differentiation needs often to compute \ensuremath{\Varid{f}_{2}\;\Varid{x}_{2}\ominus \Varid{f}_{1}\;\Varid{x}_{1}}, using a change
\ensuremath{\Varid{df}} from \ensuremath{\Varid{f}_{1}} to \ensuremath{\Varid{f}_{2}} and a change \ensuremath{\Varid{dx}} from \ensuremath{\Varid{x}_{1}} to \ensuremath{\Varid{x}_{2}}. Function changes
perform this job directly.
We discuss this point further in \cref{ssec:pointwise-changes}.

\subsection{Passing change targets}
It would be more symmetric to make function changes also take
updated input \ensuremath{\Varid{a}_{2}}, that is, have \ensuremath{\Varid{df}\;\Varid{a}_{1}\;\Varid{da}\;\Varid{a}_{2}} computes a change
from \ensuremath{\Varid{f}_{1}\;\Varid{a}_{1}} to \ensuremath{\Varid{f}_{2}\;\Varid{a}_{2}}. However, passing \ensuremath{\Varid{a}_{2}} explicitly adds no
information: the value \ensuremath{\Varid{a}_{2}} can be computed from \ensuremath{\Varid{a}_{1}} and \ensuremath{\Varid{da}} as
\ensuremath{\Varid{a}_{1}\oplus \Varid{da}}. Indeed, in various cases a function change can
compute its required output without actually computing \ensuremath{\Varid{a}_{1}\oplus \Varid{da}}. Since we expect the size of \ensuremath{\Varid{a}_{1}} and \ensuremath{\Varid{a}_{2}}
is asymptotically larger than \ensuremath{\Varid{da}}, actually computing \ensuremath{\Varid{a}_{2}} could
be expensive.\footnote{We show later efficient change structures where \ensuremath{\oplus }
reuses part of \ensuremath{\Varid{a}_{1}} to output \ensuremath{\Varid{a}_{2}} in logarithmic time.}
Hence, we stick to our asymmetric form of function
changes.

\section{Differentiation, informally}
\label{sec:informal-derive}
Next, we define differentiation and explain informally why it
does what we want. We then give an example of how differentiation
applies to our example. A formal proof will follow soon in
\cref{sec:correct-derive}, justifying more formally why this
definition is correct, but we proceed more gently.

\begin{restatable}[Differentiation]{definition}{deriveDef}
  \label{def:derive}
Differentiation is the following term transformation:
\deriveDefCore
where \ensuremath{\DeriveConst{\Varid{c}}} defines differentiation on primitives and
is provided by language plugins (see \cref{sec:lang-plugins}),
and \ensuremath{\Varid{dx}} stands for a variable generated by prefixing \ensuremath{\Varid{x}}'s
name with \ensuremath{\Varid{d}}, so that \ensuremath{\Derive{\Varid{y}}\mathrel{=}\Varid{dy}} and so on.%
\end{restatable}
If we extend the language with (non-recursive) \ensuremath{\mathbf{let}}-bindings, we can give
derived rules for it such as:
% \[
% \begin{array}{llrl}
%   |derive(lett x = t1 in t2)| ={} & \mathbf{let} & x ={} & |t1| \\
%   && \mathit{dx} ={}& |derive t1|\\
%   & \mathbf{in} && |derive t2|
% \end{array}
% \]
% |lett  x = t1|\\
%       & |dx = derive(t1)|\\
% &|in    derive(t2)|
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{29}{@{}>{\hspre}c<{\hspost}@{}}%
\column{29E}{@{}l@{}}%
\column{32}{@{}>{\hspre}l<{\hspost}@{}}%
\column{38}{@{}>{\hspre}l<{\hspost}@{}}%
\column{42}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Derive{\mathbf{let}\;\Varid{x}\mathrel{=}\Varid{t}_{1}\;\mathbf{in}\;\Varid{t}_{2}}{}\<[29]%
\>[29]{}\mathrel{=}{}\<[29E]%
\>[32]{}\mathbf{let}\;{}\<[38]%
\>[38]{}\Varid{x}{}\<[42]%
\>[42]{}\mathrel{=}\Varid{t}_{1}{}\<[E]%
\\
\>[38]{}\Varid{dx}{}\<[42]%
\>[42]{}\mathrel{=}\Derive{\Varid{t}_{1}}{}\<[E]%
\\
\>[32]{}\mathbf{in}\;{}\<[38]%
\>[38]{}\Derive{\Varid{t}_{2}}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
In \cref{sec:general-recursion} we will explain that the same transformation
rules apply for \emph{recursive} \ensuremath{\mathbf{let}}-bindings.\pg{This is not the proof, so
  we can be more informal.}

If \ensuremath{\Varid{t}} contains occurrences of both (say) \ensuremath{\Varid{x}} and \ensuremath{\Varid{dx}}, capture issues arise in
\ensuremath{\Derive{\Varid{t}}}. We defer these issues to \cref{sec:derive-binding-issues}, and
assume throughout that such issues can be avoided by $\alpha$-renaming and the
Barendregt convention \citep{Barendregt1984lambda}.

This transformation might seem deceptively simple. Indeed, pure
$\lambda$-calculus only handles binding and higher-order
functions, leaving ``real work'' to primitives. Similarly, our
transformation incrementalizes binding and higher-order
functions, leaving ``real work'' to derivatives of primitives.
However, our support of $\lambda$-calculus allows to \emph{glue}
primitives together. We'll discuss later how we add support to
various primitives and families of primitives.

Now we try to motivate the transformation informally. We claimed
that \ensuremath{\Derive{\text{\textendash}}} must satisfy \cref{slogan:derive}, which reads
%
\sloganDerive*

Let's analyze the definition of \ensuremath{\Derive{\text{\textendash}}} by case analysis
of input term \ensuremath{\Varid{u}}. In each case we assume that our slogan applies
to any subterms of \ensuremath{\Varid{u}}, and sketch why it applies to \ensuremath{\Varid{u}} itself.
\begin{itemize}
\item if \ensuremath{\Varid{u}\mathrel{=}\Varid{x}}, by our slogan \ensuremath{\Derive{\Varid{x}}} must evaluate to the
  change of \ensuremath{\Varid{x}} when inputs change, so we set \ensuremath{\Derive{\Varid{x}}\mathrel{=}\Varid{dx}}.
\item if \ensuremath{\Varid{u}\mathrel{=}\Varid{c}}, we simply delegate differentiation to
  \ensuremath{\DeriveConst{\Varid{c}}}, which is defined by plugins. Since plugins
  can define arbitrary primitives, they need provide their
  derivatives.
\item if \ensuremath{\Varid{u}\mathrel{=}\lambda \Varid{x}\to \Varid{t}}, then \ensuremath{\Varid{u}} introduces a function. Assume for
  simplicity that \ensuremath{\Varid{u}} is a closed term. Then \ensuremath{\Derive{\Varid{t}}}
  evaluates to the change of the result of this function \ensuremath{\Varid{u}},
  evaluated in a context binding \ensuremath{\Varid{x}} and its change \ensuremath{\Varid{dx}}. Then,
  because of how function changes are defined, the change of \ensuremath{\Varid{u}}
  is the change of output \ensuremath{\Varid{t}} as a function of the
  \emph{base input} \ensuremath{\Varid{x}} and its change \ensuremath{\Varid{dx}}, that is \ensuremath{\Derive{\Varid{u}}\mathrel{=}\lambda \Varid{x}\;\Varid{dx}\to \Derive{\Varid{t}}}.
\item if \ensuremath{\Varid{u}\mathrel{=}\Varid{s}\;\Varid{t}}, then \ensuremath{\Varid{s}} is a function. Assume for
  simplicity that \ensuremath{\Varid{u}} is a closed term. Then \ensuremath{\Derive{\Varid{s}}}
  evaluates to the change of \ensuremath{\Varid{s}}, as a function of \ensuremath{\Derive{\Varid{s}}}'s
  base input and input change. So, we apply \ensuremath{\Derive{\Varid{s}}} to its
  actual base input \ensuremath{\Varid{t}} and actual input change \ensuremath{\Derive{\Varid{t}}}, and
  obtain \ensuremath{\Derive{\Varid{s}\;\Varid{t}}\mathrel{=}\Derive{\Varid{s}}\;\Varid{t}\;\Derive{\Varid{t}}}.
\end{itemize}

This is not quite a correct proof sketch because of many issues,
but we fix these issues with our formal treatment in
\cref{sec:correct-derive}. In particular, in the case for
abstraction \ensuremath{\Varid{u}\mathrel{=}\lambda \Varid{x}\to \Varid{t}}, \ensuremath{\Derive{\Varid{t}}} depends not only on \ensuremath{\Varid{x}}
and \ensuremath{\Varid{dx}}, but also on other free variables of \ensuremath{\Varid{u}} and their
changes. Similarly, we must deal with free variables also in the
case for application \ensuremath{\Varid{u}\mathrel{=}\Varid{s}\;\Varid{t}}. But first, we apply
differentiation to our earlier example.

\section{Differentiation on our example}
\label{sec:derive-example}
\label{sec:derive-example-merge}
\pg{This example is still a bit too complex as written; I'm
  skipping too many steps. Unless it comes after the basic
  formalism is established.}

To exemplify the behavior of differentiation concretely, and help
fix ideas for later discussion, in this section we show how the derivative of
\ensuremath{\Varid{grandTotal}} looks like.

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{14}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{grandTotal}{}\<[14]%
\>[14]{}\mathrel{=}\lambda \Varid{xs}\;\Varid{ys}\to \Varid{sum}\;(\Varid{merge}\;\Varid{xs}\;\Varid{ys}){}\<[E]%
\\
\>[B]{}\Varid{s}{}\<[14]%
\>[14]{}\mathrel{=}\Varid{grandTotal}\;\{\mskip1.5mu \{\mskip1.5mu \mathrm{1}\mskip1.5mu\}\mskip1.5mu\}\;\{\mskip1.5mu \{\mskip1.5mu \mathrm{2},\mathrm{3},\mathrm{4}\mskip1.5mu\}\mskip1.5mu\}\mathrel{=}\mathrm{11}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
Differentiation is a structurally recursive program transformation,
so we first differentiate \ensuremath{\Varid{merge}\;\Varid{xs}}\linebreak\ensuremath{\Varid{ys}}. To compute its change
we simply call the derivative of \ensuremath{\Varid{merge}}, that is \ensuremath{\Varid{dmerge}}, and
apply it to the base inputs and their changes: hence we write
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Derive{\Varid{merge}\;\Varid{xs}\;\Varid{ys}}\mathrel{=}\Varid{dmerge}\;\Varid{xs}\;\Varid{dxs}\;\Varid{ys}\;\Varid{dys}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
As we'll
better see later, we can define function \ensuremath{\Varid{dmerge}} as
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{dmerge}\mathrel{=}\lambda \Varid{xs}\;\Varid{dxs}\;\Varid{ys}\;\Varid{dys}\to \Varid{merge}\;\Varid{dxs}\;\Varid{dys}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
%
so \ensuremath{\Derive{\Varid{merge}\;\Varid{xs}\;\Varid{ys}}} can be simplified by $\beta$-reduction
to \ensuremath{\Varid{merge}\;\Varid{dxs}\;\Varid{dys}}:
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{13}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[13]{}\Derive{\Varid{merge}\;\Varid{xs}\;\Varid{ys}}{}\<[E]%
\\
\>[3]{}={}\<[13]%
\>[13]{}\Varid{dmerge}\;\Varid{xs}\;\Varid{dxs}\;\Varid{ys}\;\Varid{dys}{}\<[E]%
\\
\>[3]{}=_{\beta}{}\<[13]%
\>[13]{}(\lambda \Varid{xs}\;\Varid{dxs}\;\Varid{ys}\;\Varid{dys}\to \Varid{merge}\;\Varid{dxs}\;\Varid{dys})\;\Varid{xs}\;\Varid{dxs}\;\Varid{ys}\;\Varid{dys}{}\<[E]%
\\
\>[3]{}=_{\beta}{}\<[13]%
\>[13]{}\Varid{merge}\;\Varid{dxs}\;\Varid{dys}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks

Let's next derive \ensuremath{\Varid{sum}\;(\Varid{merge}\;\Varid{xs}\;\Varid{ys})}. First, like above, the
derivative of \ensuremath{\Varid{sum}\;\Varid{zs}} would be \ensuremath{\Varid{dsum}\;\Varid{zs}\;\Varid{dzs}}, which depends on
base input \ensuremath{\Varid{zs}} and its change \ensuremath{\Varid{dzs}}. As we'll see, \ensuremath{\Varid{dsum}\;\Varid{zs}\;\Varid{dzs}}
can simply call \ensuremath{\Varid{sum}} on \ensuremath{\Varid{dzs}}, so \ensuremath{\Varid{dsum}\;\Varid{zs}\;\Varid{dzs}\mathrel{=}\Varid{sum}\;\Varid{dzs}}. To
derive \ensuremath{\Varid{sum}\;(\Varid{merge}\;\Varid{xs}\;\Varid{ys})}, we must call the derivative of \ensuremath{\Varid{sum}},
that is \ensuremath{\Varid{dsum}}, on its base argument and its change, so on \ensuremath{\Varid{merge}\;\Varid{xs}\;\Varid{ys}} and \ensuremath{\Derive{\Varid{merge}\;\Varid{xs}\;\Varid{ys}}}. We can later simplify again by
$\beta$-reduction and obtain
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{13}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[13]{}\Derive{\Varid{sum}\;(\Varid{merge}\;\Varid{xs}\;\Varid{ys})}{}\<[E]%
\\
\>[3]{}={}\<[13]%
\>[13]{}\Varid{dsum}\;(\Varid{merge}\;\Varid{xs}\;\Varid{ys})\;\Derive{\Varid{merge}\;\Varid{xs}\;\Varid{ys}}{}\<[E]%
\\
\>[3]{}=_{\beta}{}\<[13]%
\>[13]{}\Varid{sum}\;\Derive{\Varid{merge}\;\Varid{xs}\;\Varid{ys}}{}\<[E]%
\\
\>[3]{}={}\<[13]%
\>[13]{}\Varid{sum}\;(\Varid{dmerge}\;\Varid{xs}\;\Varid{dxs}\;\Varid{ys}\;\Varid{dys}){}\<[E]%
\\
\>[3]{}=_{\beta}{}\<[13]%
\>[13]{}\Varid{sum}\;(\Varid{merge}\;\Varid{dxs}\;\Varid{dys}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks

Here we see the output of differentiation is defined in a bigger
typing context: while \ensuremath{\Varid{merge}\;\Varid{xs}\;\Varid{ys}} only depends on base inputs
\ensuremath{\Varid{xs}} and \ensuremath{\Varid{ys}}, \ensuremath{\Derive{\Varid{merge}\;\Varid{xs}\;\Varid{ys}}} also depends on their
changes. This property extends beyond the examples we just saw:
if a term \ensuremath{\Varid{t}} is defined in context \ensuremath{\Gamma}, then the output of
derivation \ensuremath{\Derive{\Varid{t}}} is defined in context \ensuremath{\Gamma,\Delta \Gamma},
where \ensuremath{\Delta \Gamma} is a context that binds a change \ensuremath{\Varid{dx}} for each
base input \ensuremath{\Varid{x}} bound in the context \ensuremath{\Gamma}.

Next we consider \ensuremath{\lambda \Varid{xs}\;\Varid{ys}\to \Varid{sum}\;(\Varid{merge}\;\Varid{xs}\;\Varid{ys})}.
Since variables \ensuremath{\Varid{xs},\Varid{dxs},\Varid{ys},\Varid{dys}} appear free in \ensuremath{\Derive{\Varid{sum}\;(\Varid{merge}\;\Varid{xs}\;\Varid{ys})}} (ignoring later
optimizations), term
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Derive{\lambda \Varid{xs}\;\Varid{ys}\to \Varid{sum}\;(\Varid{merge}\;\Varid{xs}\;\Varid{ys})}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
must bind all those variables.

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{13}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[13]{}\Derive{\lambda \Varid{xs}\;\Varid{ys}\to \Varid{sum}\;(\Varid{merge}\;\Varid{xs}\;\Varid{ys})}{}\<[E]%
\\
\>[3]{}={}\<[13]%
\>[13]{}\lambda \Varid{xs}\;\Varid{dxs}\;\Varid{ys}\;\Varid{dys}\to \Derive{\Varid{sum}\;(\Varid{merge}\;\Varid{xs}\;\Varid{ys})}{}\<[E]%
\\
\>[3]{}=_{\beta}{}\<[13]%
\>[13]{}\lambda \Varid{xs}\;\Varid{dxs}\;\Varid{ys}\;\Varid{dys}\to \Varid{sum}\;(\Varid{merge}\;\Varid{dxs}\;\Varid{dys}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks

Next we need to transform the binding of \ensuremath{\Varid{grandTotal}} to its body \ensuremath{\Varid{b}\mathrel{=}\lambda \Varid{xs}\;\Varid{ys}\to \Varid{sum}\;(\Varid{merge}\;\Varid{xs}\;\Varid{ys})}. We copy this binding and add a new additional binding from \ensuremath{\Varid{dgrandTotal}} to the derivative of \ensuremath{\Varid{b}}.

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{14}{@{}>{\hspre}l<{\hspost}@{}}%
\column{26}{@{}>{\hspre}l<{\hspost}@{}}%
\column{34}{@{}>{\hspre}c<{\hspost}@{}}%
\column{34E}{@{}l@{}}%
\column{38}{@{}>{\hspre}l<{\hspost}@{}}%
\column{43}{@{}>{\hspre}l<{\hspost}@{}}%
\column{51}{@{}>{\hspre}l<{\hspost}@{}}%
\column{56}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{grandTotal}{}\<[14]%
\>[14]{}\mathrel{=}\lambda \Varid{xs}\;{}\<[26]%
\>[26]{}\Varid{ys}{}\<[34]%
\>[34]{}\to {}\<[34E]%
\>[38]{}\Varid{sum}\;{}\<[43]%
\>[43]{}(\Varid{merge}\;{}\<[51]%
\>[51]{}\Varid{xs}\;{}\<[56]%
\>[56]{}\Varid{ys}){}\<[E]%
\\
\>[B]{}\Varid{dgrandTotal}{}\<[14]%
\>[14]{}\mathrel{=}\lambda \Varid{xs}\;\Varid{dxs}\;{}\<[26]%
\>[26]{}\Varid{ys}\;\Varid{dys}{}\<[34]%
\>[34]{}\to {}\<[34E]%
\>[38]{}\Varid{sum}\;{}\<[43]%
\>[43]{}(\Varid{merge}\;{}\<[51]%
\>[51]{}\Varid{dxs}\;{}\<[56]%
\>[56]{}\Varid{dys}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks

Finally, we need to transform the binding of \ensuremath{\Varid{output}} and its body. By iterating similar steps,
in the end we get:
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{15}{@{}>{\hspre}l<{\hspost}@{}}%
\column{25}{@{}>{\hspre}l<{\hspost}@{}}%
\column{35}{@{}>{\hspre}l<{\hspost}@{}}%
\column{38}{@{}>{\hspre}l<{\hspost}@{}}%
\column{43}{@{}>{\hspre}c<{\hspost}@{}}%
\column{43E}{@{}l@{}}%
\column{47}{@{}>{\hspre}l<{\hspost}@{}}%
\column{52}{@{}>{\hspre}l<{\hspost}@{}}%
\column{56}{@{}>{\hspre}l<{\hspost}@{}}%
\column{60}{@{}>{\hspre}l<{\hspost}@{}}%
\column{65}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{grandTotal}{}\<[15]%
\>[15]{}={}\<[25]%
\>[25]{}\lambda \Varid{xs}\;{}\<[35]%
\>[35]{}\Varid{ys}{}\<[43]%
\>[43]{}\to {}\<[43E]%
\>[47]{}\Varid{sum}\;{}\<[52]%
\>[52]{}(\Varid{merge}\;{}\<[60]%
\>[60]{}\Varid{xs}\;{}\<[65]%
\>[65]{}\Varid{ys}){}\<[E]%
\\
\>[B]{}\Varid{dgrandTotal}{}\<[15]%
\>[15]{}={}\<[25]%
\>[25]{}\lambda \Varid{xs}\;\Varid{dxs}\;{}\<[35]%
\>[35]{}\Varid{ys}\;\Varid{dys}{}\<[43]%
\>[43]{}\to {}\<[43E]%
\>[47]{}\Varid{sum}\;{}\<[52]%
\>[52]{}(\Varid{merge}\;{}\<[60]%
\>[60]{}\Varid{dxs}\;{}\<[65]%
\>[65]{}\Varid{dys}){}\<[E]%
\\
\>[B]{}\Varid{s}{}\<[15]%
\>[15]{}={}\<[25]%
\>[25]{}\Varid{grandTotal}\;{}\<[38]%
\>[38]{}\{\mskip1.5mu \{\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu\}\mskip1.5mu\}\;{}\<[56]%
\>[56]{}\{\mskip1.5mu \{\mskip1.5mu \mathrm{4}\mskip1.5mu\}\mskip1.5mu\}{}\<[E]%
\\
\>[B]{}\Varid{ds}{}\<[15]%
\>[15]{}={}\<[25]%
\>[25]{}\Varid{dgrandTotal}\;{}\<[38]%
\>[38]{}\{\mskip1.5mu \{\mskip1.5mu \mathrm{1},\mathrm{2},\mathrm{3}\mskip1.5mu\}\mskip1.5mu\}\;\{\mskip1.5mu \{\mskip1.5mu \mathrm{1}\mskip1.5mu\}\mskip1.5mu\}\;\{\mskip1.5mu \{\mskip1.5mu \mathrm{4}\mskip1.5mu\}\mskip1.5mu\}\;\{\mskip1.5mu \{\mskip1.5mu \mathrm{5}\mskip1.5mu\}\mskip1.5mu\}{}\<[E]%
\\
\>[15]{}=_{\beta}{}\<[25]%
\>[25]{}\Varid{sum}\;(\Varid{merge}\;\{\mskip1.5mu \{\mskip1.5mu \mathrm{1}\mskip1.5mu\}\mskip1.5mu\}\;\{\mskip1.5mu \{\mskip1.5mu \mathrm{5}\mskip1.5mu\}\mskip1.5mu\}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks

\paragraph{Self-maintainability}
Differentiation does not always produce efficient derivatives
without further program transformations; in particular,
derivatives might need to recompute results produced by the base
program. In the above example, if we don't inline derivatives and
use $\beta$-reduction to simplify programs, \ensuremath{\Derive{\Varid{sum}\;(\Varid{merge}\;\Varid{xs}\;\Varid{ys})}} is just \ensuremath{\Varid{dsum}\;(\Varid{merge}\;\Varid{xs}\;\Varid{ys})\;\Derive{\Varid{merge}\;\Varid{xs}\;\Varid{ys}}}. A
direct execution of this program will compute \ensuremath{\Varid{merge}\;\Varid{xs}\;\Varid{ys}},
which would waste time linear in the base inputs.%
% \pg{Point out this is self-maintainable!}

% Without such optimizations, incremental program
% |dsum (merge xs ys) (derive(merge xs ys))| would recompute |merge
% xs ys|, which would be wasteful.
We'll show how to avoid such recomputations in general in
\cref{sec:cts-motivation}; but here we can avoid computing \ensuremath{\Varid{merge}\;\Varid{xs}\;\Varid{ys}} simply
because \ensuremath{\Varid{dsum}} does not use its base argument, that is, it is
\emph{self-maintainable}. Without the approach described in
\cref{ch:cts}, we are restricted to self-maintainable derivatives.

% Revise title
\section{Conclusion and contributions}
\label{sec:incr-contributions}
In this chapter, we have seen how a correct differentiation transform
allows us to incrementalize programs.

\subsection{Navigating this thesis part}
\label{sec:navigating-incr}

\paragraph{Differentiation}
This chapter and \crefrange{ch:diff-examples}{ch:term-reasoning} form the core
of incrementalization theory, with other chapters building on top of them.
We study incrementalization for STLC; we summarize our formalization of STLC in
\cref{sec:preliminaries}.
Together with \cref{sec:intro}, \cref{ch:diff-examples} introduces the overall
approach to incrementalization informally.
\Cref{ch:derive-formally,ch:change-theory} show that incrementalization using
ILC is correct.
Building on a set-theoretic semantics, these
\lcnamecrefs{ch:derive-formally} also develop the theory
underlying this correctness proofs and further results.
Equational reasoning on terms is then developed
in \cref{ch:term-reasoning}.

\Crefrange{ch:derive-formally}{ch:term-reasoning} contain full
formal proofs: readers are welcome to skip or skim those proofs where
appropriate.
For ease of reference and to help navigation and skimming, these highlight and
number all definitions, notations, theorem statements and so on, and we strive
not to hide important definitions inside theorems.

Later chapters build on this core but are again independent from each other.

\paragraph{Extensions and theoretical discussion}
\Cref{ch:misc-extensions} discusses a few assorted aspects of the theory that do
not fit elsewhere and do not suffice for standalone chapters. We
show how to differentiation general recursion~\cref{sec:general-recursion}, we
exhibit a function change that is not valid for any function
(\cref{sec:very-invalid}), we contrast our representation of function changes
with \emph{pointwise} function changes (\cref{ssec:pointwise-changes}), and we
compare our formalization with the one presented in \citep{CaiEtAl2014ILC}
(\cref{sec:alt-change-validity}).

\paragraph{Performance of differentiated programs}
\Cref{sec:applying} studies how to apply differentiation (as introduced in
previous chapters) to incrementalize a case study and empirically evaluates
performance speedups.

\paragraph{Differentiation with cache-transfer-style conversion}
\Cref{ch:cts} is self-contained, even though it builds on the rest of the
material; it summarizes the basics of ILC in \cref{sec:ilc-background}.
Terminology in that chapter is sometimes subtly different from the rest of the
thesis.

\paragraph{Towards differentiation for System F}
\Cref{ch:diff-parametricity-system-f} outlines how to extend differentiation to
System F and suggests a proof avenue. Differentiation for System F requires a
generalization of change structures to changes across different types
(\cref{sec:param-derive-changes-across-types}) that appears of independent
interest, though further research remains to be done.

\paragraph{Related work, conclusions and appendixes}
Finally, \cref{sec:cts-rw,ch:incr-rw} discuss related work and
\cref{ch:incr-conclusion} concludes this part of the thesis.

\subsection{Contributions}
In \crefrange{ch:diff-examples}{ch:term-reasoning} and \cref{sec:applying} we make the following contributions:
\begin{itemize}
\item We present a novel mathematical theory of changes and derivatives, which
  is more general than other work in the field because changes are first-class
  entities, they are distinct from base values and they are defined also for
  functions.
  %(\cref{sec:1st-order-changes}).
  %KO: I think the next sentence cannot be understood at this point.
  %We introduce changes for complex types, defined compositionally.
%
\item We present the first approach to incremental computation for pure
  $\lambda$-calculi by a source-to-source transformation, $\DERIVE$, that
  requires no run-time support. The transformation produces an incremental
  program in the same language; all optimization techniques for the original
  program are applicable to the incremental program as well.

  % \pg{ Maybe comment in? Not sure, which might be telling.}
  % Since our incremental programs use no impure features, they are
  % especially amenable to further optimizations, making this approach
  % very suitable for further research.

%KO: commented this out. I think the purity is not important enough
%to deserve another sentence here, since we only vaguely hint
%at "further research".
% KO: Let's have one bullet point per section. Also, a conjecture
% sounds like a rather weak contribution
%\item We argue that incrementalization is efficient on
%  \emph{self-maintainable programs}, and discuss how further research on
%  static or dynamic memoization can speed up a larger class of programs (\cref{sec:performance-cons}).
%  \pg{This contribution references text which is now commented
%    out. I believe the text should be brought back in.}
%
\item We prove that our incrementalizing transformation $\DERIVE$
is correct by a novel machine-checked logical relation
proof, mechanized in Agda.

\item While we focus mainly on the theory of changes
and derivatives, we also perform a performance case study.
We implement the derivation transformation in Scala,
with a plug-in architecture that can be extended with new base
types and primitives. We define a plugin with support for
different collection types and use the plugin to
incrementalize a variant of the MapReduce programming model~\citep{Lammel07}.
  Benchmarks show that on this program,
  incrementalization can reduce asymptotic complexity and can turn $O(n)$
  performance into $O(1)$, improving running time by over 4
  orders of magnitude on realistic inputs (\cref{sec:applying}).
\end{itemize}

In \cref{ch:cts} we make the following contributions:
% altered copy of contribution list in that chapter.
\begin{itemize}
\item via examples, we motivate extending ILC to remember intermediate
  results (\cref{sec:cts-motivation});
\item we give a novel proof of correctness for ILC for untyped
  $\lambda$-calculus, based on step-indexed logical relations
  (\cref{sec:sound-derive});
\item building on top of ILC-style differentiation, we show how to transform
  untyped higher-order programs to \emph{cache-transfer-style (CTS)}
  (\cref{sec:transformation});
\item we show through formal proofs that programs and derivatives in cache-transfer style
  \emph{simulate} correctly their non-CTS variants (\cref{sec:transformation-soundness});
\item we perform performance case studies (in \cref{sec:case-studies}) applying
  (by hand) extension of this technique to Haskell programs, and incrementalize
  efficiently also programs that do not admit self-maintainable derivatives.
\end{itemize}

\Cref{ch:diff-parametricity-system-f} describes how to extend differentiation to
System F. To this end, we extend change structure to allow from changes where
source and destination have different types and enable defining more powerful
combinators for change structures to be more powerful.
While the results in this chapter call for further research, we consider them
exciting

\Cref{ch:bsos} proposes the first correctness proofs for ILC via operational
methods and (step-indexed) logical relations, for simply-typed $\lambda$-calculus
(without and with general recursion) and for untyped $\lambda$-calculus.
A later variant of this proof, adapted for use of cache-transfer-style, is
presented in \cref{ch:cts}, but we believe the presentation in \cref{ch:bsos}
might also be interesting for theorists, as it explains how to extend ILC
correctness proofs with fewer extraneous complications.
Nevertheless, given the similarity between proofs in \cref{ch:bsos,ch:cts}, we
relegate the latter one to an appendix.

Finally, \cref{ch:defunc-fun-changes} shows how to implement all change operations on
function changes efficiently, by defunctionalizing functions and
function changes rather than using mere closure conversion.
