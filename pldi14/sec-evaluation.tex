% Emacs, this is -*- latex -*-!
\section{Benchmarks}
% Disabled since we shouldn't refer to this appendix, it's just
% part of the extended version of the paper.
%\label{sec:benchmarks}

Benchmarking our case study shows
that \ILC\ can offer order-of-magnitude speedups for a
realistic higher-order program.

\begin{oldSec} % ID=Adaptation
We performed additional microbenchmarks on simpler examples
(summing the values of a map\pg{Cai: why not the elements of a
  bag?}, mapping the successor function on bag elements, bag
merge);\pg{the examples are really small, is it OK? I guess so.}
results on those examples did not yield additional insight, hence
we omit them from this report.\pg{Add the links for details?}

\paragraph{Adaptation}
We first adapted our example program to our object language,
as parameterized by the plugin we presented in
Sec.~\ref{sec:plugins}. Where primitives were insufficient, we
added new ones as appropriate~\pg{Ensure we don't contradict this
  claim elsewhere.}.

First, we adapted the generic MapReduce framework as a library of
polymorphic object-level functions. As usual, we use the
meta-language both to define the library and to encode
polymorphism.

This produces a more restrictive variant of MapReduce. \pg{Explain.}

Future work, supporting non-self-incrementalizable primitives through caching, would allow lifting these limitations.

The result is shown in \cref{fig:case-study-pseudocode}.
\end{oldSec}

\paragraph{Benchmarking setup}
\pg{Omit this at least while we lack space.}
\begin{oldSec}
We ran benchmarks on an 8-core Intel Core i7-2600 CPU running at
3.4 GHz, with 8GB of RAM, running Scientific Linux release 6.4.
While the benchmark code is single-threaded, the JVM offloads
garbage collection to other cores. We use the preinstalled
OpenJDK 1.7.0\_25 and Scala 2.10.2.
\end{oldSec}

We run object language programs by generating corresponding Scala code.
To ensure rigorous
benchmarking~\citep{Georges07rigorousJavaPerformance}, we use the
Scalameter benchmarking library. To show that the performance
difference from the baseline is statistically significant, we
show 99\%-confidence intervals in graphs.

We verify \cref{eq:correctness} experimentally
by checking that the two sides of the equation always
evaluate to the same value.

\paragraph{Input generation}

Inputs are randomly generated to resemble English words over all
webpages on the internet: The vocabulary size and the average
length of a webpage stay relatively the same, while the number of
webpages grows day by day. To generate a size-$n$ input of type
$(\HashMap{\Int}{\Bag*[\Int]})$, we generate $n$ random numbers
between 1 and 1000 and distribute them randomly in $n/1000$ bags.
Changes are randomly generated to resemble edits. A change has
50\% probability to delete a random existing number, and has 50\%
probability to insert a random number at a random location.

\paragraph{Experimental units}

Thanks to \cref{eq:correctness}, both recomputation
$\App{f}{\Apply*{\D a}{a}}$ and incremental computation
$\Apply{\App{\App{\Derive{f}}{a}}{\D a}}{\App{f}{a}}$ produce
the same result. Both programs are written in our object language.
To show that derivatives are faster, we compare
these two computations. To compare with recomputation, we measure the
\emph{aggregated} running time for running the derivative on the change 
and for updating the original output with the result of the derivative.

\subsection{Experimental results}

\pg{Warning: benchmark results are hardcoded here, copy-pasted
  from the spreadsheet. Edit with care.}

We present our results in \cref{fig:graph}. As expected, the
runtime of incremental computation is
\emph{essentially constant} in the size of the input, while the runtime
of recomputation is linear in the input size.
Hence, on our biggest inputs\pg{specify} incremental computation
is over $10^4$ times faster.

Derivative time is in fact slightly irregular for the first few inputs,
but this irregularity decreases slowly with increasing warmup
cycles. In the end, for derivatives we use $10^4$ warmup cycles.
With fewer warmup cycles, running time for derivatives decreases
significantly during execution, going from 2.6ms for $n = 1000$ to
0.2ms for $n = 512000$. Hence, we believe extended warmup is
appropriate, and the changes do not affect our general
conclusions. Considering confidence intervals, in our experiments the running
time for derivatives varies between 0.139ms and 0.378ms.

In our current implementation, the code of the generated derivatives can become 
quite big. For the histogram example (which is around 1KB of code), a pretty-print
of its derivative is around 40KB of code. The function application case
in \cref{fig:correctness:derive} can lead to a quadratic growth in the 
worst case. We believe that the code size of the derivative can be reduced again 
by common subexpression elimination, but we did not yet pursue that option.

\paragraph{Summary}
Our results show that the incrementalized program runs
in essentially constant time and hence orders of magnitude faster than
the alternative of recomputation from scratch. 
